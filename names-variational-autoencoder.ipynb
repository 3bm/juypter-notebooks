{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = set()\n",
    "for filename in ['male.txt', 'female.txt']:\n",
    "    for line in open(os.path.join('../data/names', filename)):\n",
    "        if len(line.strip()):\n",
    "            names.add(line.strip().lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7583 names\n",
      "longest: 54\n",
      "99th percentile longest: 9\n"
     ]
    }
   ],
   "source": [
    "print len(names), 'names'\n",
    "print 'longest:', max(map(len, names))\n",
    "by_len = sorted(names, key=len)\n",
    "print '99th percentile longest:', len(by_len[int(len(names) * 0.95)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13  0 19  4 26 27 27 27 27 27]\n"
     ]
    }
   ],
   "source": [
    "chars = list('abcdefghijklmnopqrstuvwxyz') + ['<END>', '<NULL>']\n",
    "indices_for_chars = {c: i for i, c in enumerate(chars)}\n",
    "\n",
    "NAME_MAX_LEN = 10 # include the <END> char\n",
    "\n",
    "def name_to_vec(name, maxlen=NAME_MAX_LEN):\n",
    "    v = np.zeros(maxlen, dtype=int)\n",
    "    null_idx = indices_for_chars['<NULL>']\n",
    "    v.fill(null_idx)\n",
    "    for i, c in enumerate(name):\n",
    "        if i >= maxlen: break\n",
    "        n = indices_for_chars.get(c, null_idx)\n",
    "        v[i] = n\n",
    "    v[min(len(name), maxlen-1)] = indices_for_chars['<END>']\n",
    "    return v\n",
    "\n",
    "def vec_to_name(vec):\n",
    "    name = ''\n",
    "    for x in vec:\n",
    "        char = chars[x]\n",
    "        if len(char) == 1:\n",
    "            name += char\n",
    "        elif char == '<END>':\n",
    "            return name\n",
    "    return name\n",
    "\n",
    "print name_to_vec('nate')\n",
    "assert vec_to_name(name_to_vec('nate')) == 'nate'\n",
    "assert vec_to_name(name_to_vec('aaaaaaaaaaaa')) == 'aaaaaaaaa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7583, 10)\n"
     ]
    }
   ],
   "source": [
    "name_vecs = np.array([name_to_vec(n) for n in names])\n",
    "print name_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_var(shape, stddev=0.1, weight_decay=0, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    v = tf.Variable(initial, name=name)\n",
    "    if weight_decay > 0:\n",
    "        l2 = tf.nn.l2_loss(v) * weight_decay\n",
    "        tf.add_to_collection('losses', l2)\n",
    "    return v\n",
    "\n",
    "def leaky_relu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "def relu(x):\n",
    "    # return tf.nn.relu(x)\n",
    "    return leaky_relu(x)\n",
    "\n",
    "def create_conv(input, out_channels, patch_size=5, stride=1, batch_norm=False, dropout=False):\n",
    "    in_channels = input.get_shape()[-1].value\n",
    "    w = weight_var([patch_size, patch_size, in_channels, out_channels])\n",
    "    b = weight_var([out_channels], stddev=0)\n",
    "    conv = tf.nn.conv2d(input, w, strides=[1,stride,stride,1], padding='SAME')\n",
    "    if batch_norm: conv = create_batch_norm(conv)\n",
    "    activation = relu(conv + b)\n",
    "    if dropout: activation = create_dropout(activation)\n",
    "    return activation\n",
    "    \n",
    "def text_conv(input, out_channels, patch_size=5, stride=1, dropout=False, pool_size=1):\n",
    "    in_channels = input.get_shape()[-1].value\n",
    "    w = weight_var([patch_size, in_channels, out_channels])\n",
    "    b = weight_var([out_channels], stddev=0)\n",
    "    conv = tf.nn.conv1d(input, w, stride=stride, padding='SAME')\n",
    "    activation = relu(conv + b)\n",
    "    # TODO: max_pooling\n",
    "    if dropout: activation = create_dropout(activation)\n",
    "    return activation\n",
    "\n",
    "def create_dropout(units):\n",
    "    return tf.nn.dropout(units, dropout)\n",
    "\n",
    "def create_fc(input, out_size):\n",
    "    # input_dropped = tf.nn.dropout(input, dropout_keep_prob)\n",
    "    in_size = input.get_shape()[-1].value\n",
    "    w = weight_var([in_size, out_size], weight_decay=0.004)\n",
    "    b = weight_var([out_size], weight_decay=0.004)\n",
    "    x = tf.matmul(input, w)\n",
    "    return relu(x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_placeholder = tf.placeholder(shape=[None, NAME_MAX_LEN], dtype=tf.int32, name='names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z_SIZE = 64\n",
    "\n",
    "def encoder_lstm(names):\n",
    "    with tf.variable_scope('encoder'):\n",
    "        cells = [tf.nn.rnn_cell.LSTMCell(size, state_is_tuple=True) for size in [len(chars), 64]]\n",
    "        lstm = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
    "        one_hot = tf.one_hot(names, len(chars), dtype=tf.float32)\n",
    "        outputs, state = tf.nn.dynamic_rnn(lstm, one_hot, dtype=tf.float32)\n",
    "        outputs_flat = tf.reshape(outputs, [-1, 64 * NAME_MAX_LEN])\n",
    "        z_mean = create_fc(outputs_flat, Z_SIZE)\n",
    "        z_stddev = create_fc(outputs_flat, Z_SIZE)\n",
    "        return z_mean, z_stddev\n",
    "\n",
    "def encoder_conv(names):\n",
    "    with tf.variable_scope('encoder'):\n",
    "        one_hot = tf.one_hot(names, len(chars), dtype=tf.float32)\n",
    "        conv1 = text_conv(one_hot, 64)\n",
    "        conv2 = text_conv(one_hot, 64)\n",
    "        fc1 = create_fc(tf.reshape(conv2, [-1, NAME_MAX_LEN * 64]), 128)\n",
    "        z_mean = create_fc(fc1, Z_SIZE)\n",
    "        z_stddev = create_fc(fc1, Z_SIZE)\n",
    "        return z_mean, z_stddev\n",
    "    \n",
    "# def generator(noise, name='generator'):\n",
    "#     with tf.variable_scope(name, reuse=None):\n",
    "#         cells = [tf.nn.rnn_cell.LSTMCell(size, state_is_tuple=True) for size in [NOISE_SIZE, 256, len(chars)]]\n",
    "#         lstm = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
    "#         noise_repeated_over_time = tf.tile(tf.reshape(noise, [-1, 1, NOISE_SIZE]), [1, NAME_MAX_LEN, 1])\n",
    "#         outputs, state = tf.nn.dynamic_rnn(lstm, noise_repeated_over_time, dtype=tf.float32)\n",
    "#         output_chars = tf.reshape(tf.argmax(tf.nn.softmax(outputs), axis=2), [-1, NAME_MAX_LEN])\n",
    "#         output_chars = tf.cast(output_chars, tf.int32)\n",
    "#     return output_chars\n",
    "\n",
    "# generated_names = generator(noise)\n",
    "\n",
    "z_mean, z_stddev = encoder_lstm(name_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00247853  0.02004284 -0.0181652  -0.04191967  0.05001117 -0.01602871\n",
      "   0.02587532  0.13481548  0.07789069 -0.02169716  0.08349291 -0.0429353\n",
      "  -0.03166916 -0.01963357  0.02664084 -0.00100241  0.03409065  0.08553927\n",
      "   0.07906856  0.00153807  0.31431997 -0.02886212  0.03868855  0.25834271\n",
      "  -0.03920429  0.0406871   0.04773729  0.029155    0.05626151 -0.00365246\n",
      "  -0.00656544  0.02019122 -0.00066316 -0.02825847  0.0101981   0.18162087\n",
      "   0.05051333  0.15712184  0.11209555 -0.01296621 -0.01769583 -0.01094536\n",
      "   0.06383231  0.06080098  0.18247971  0.02820658  0.0322004   0.09901164\n",
      "  -0.01733317 -0.01050556 -0.02658109 -0.03360737  0.07080974  0.04814442\n",
      "  -0.00804588 -0.02840882 -0.02741902  0.01266014 -0.01211043  0.04052842\n",
      "   0.09770438 -0.00944409  0.17789587  0.03068392]]\n"
     ]
    }
   ],
   "source": [
    "print session.run(z_mean, feed_dict={name_placeholder: [name_to_vec('nate')]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(z_mean, z_stddev):\n",
    "    samples = tf.random_normal(tf.shape(z_stddev), 0, 1, dtype=tf.float32)\n",
    "    return z_mean + samples * z_stddev\n",
    "\n",
    "z_vals = sample_z(z_mean, z_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decoder_lstm(z):\n",
    "    z_repeated_over_time = tf.tile(tf.reshape(z, [-1, 1, Z_SIZE]), [1, NAME_MAX_LEN, 1])\n",
    "    cells = [tf.nn.rnn_cell.LSTMCell(size, state_is_tuple=True) for size in [Z_SIZE, 256, len(chars)]]\n",
    "    lstm = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    outputs, state = tf.nn.dynamic_rnn(lstm, z_repeated_over_time, dtype=tf.float32)\n",
    "    return outputs\n",
    "\n",
    "z_input = tf.placeholder(tf.float32, [None, Z_SIZE], name='z_input')\n",
    "use_z_input = tf.placeholder(tf.int32, shape=[], name=\"use_z_input_condition\")\n",
    "decoder_input = tf.cond(use_z_input > 0, lambda: z_input, lambda: z_vals)\n",
    "\n",
    "decoded = decoder_lstm(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(decoded, name_placeholder))\n",
    "kl_divergence = tf.reduce_mean(0.5 * tf.reduce_sum(tf.square(z_mean) + tf.square(z_stddev) - tf.log(tf.square(z_stddev)) - 1, 1))\n",
    "loss = diff_loss + kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoded_vecs = tf.argmax(decoded, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learn_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "optimizer = tf.train.AdamOptimizer(learn_rate)\n",
    "global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "train_step = optimizer.minimize(loss, global_step=global_step)\n",
    "session.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from checkpoint models/nva2/model.ckpt-60600\n"
     ]
    }
   ],
   "source": [
    "save_path = 'models/nva2'\n",
    "\n",
    "session = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "session.run(init_op)\n",
    "\n",
    "saver = None\n",
    "if save_path:\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(save_path)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(session, ckpt.model_checkpoint_path)\n",
    "        print 'Restored from checkpoint', ckpt.model_checkpoint_path\n",
    "    else:\n",
    "        print 'Did not restore from checkpoint'\n",
    "else:\n",
    "    print 'Will not save progress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = False\n",
    "while train:\n",
    "    names = name_vecs[np.random.randint(name_vecs.shape[0], size=64), :]\n",
    "    feed_dict = {\n",
    "        name_placeholder: names,\n",
    "        z_input: np.zeros((64, Z_SIZE)),\n",
    "        use_z_input: 0,\n",
    "        learn_rate: 0.001\n",
    "    }\n",
    "    _, loss_, step_ = session.run([train_step, loss, global_step], feed_dict=feed_dict)\n",
    "    if step_ % 200 == 0:\n",
    "        output_ = session.run(decoded_vecs, feed_dict=feed_dict)\n",
    "        print \"Step: {0}; loss: {1}\".format(step_, loss_)\n",
    "        # print names[0]\n",
    "        # print output_[0]\n",
    "        print \" example encoding: {} -> {}\".format(vec_to_name(names[0]), vec_to_name(output_[0]))\n",
    "        if step_ % 600 == 0:\n",
    "            saver.save(session, save_path + '/model.ckpt', global_step=step_)\n",
    "            print 'Saved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nate -> nate\n"
     ]
    }
   ],
   "source": [
    "def reconstruct(name):\n",
    "    feed_dict = {\n",
    "        name_placeholder: np.array([name_to_vec(name)]),\n",
    "        z_input: np.zeros((64, Z_SIZE)),\n",
    "        use_z_input: 0,\n",
    "        learn_rate: 0.01\n",
    "    }\n",
    "    output_ = session.run(decoded_vecs, feed_dict=feed_dict)\n",
    "    return vec_to_name(output_[0])\n",
    "\n",
    "for name in ['nate']:\n",
    "    print name, '->', reconstruct(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nate -> nate\n",
      "will -> will\n",
      "chen -> chen\n",
      "atty -> atty\n",
      "arielle -> arielle\n",
      "nathaniel -> nathaniel\n",
      "kimberly -> kimberly\n",
      "erica -> eriaa\n",
      "zoe -> noe\n"
     ]
    }
   ],
   "source": [
    "# reconstruct is pretty good at reconstructing american names, even long ones:\n",
    "for name in ['nate', 'will', 'chen', 'atty', 'arielle', 'nathaniel', 'kimberly', 'erica', 'zoe']:\n",
    "    print name, '->', reconstruct(name)\n",
    "\n",
    "# although notably, it's bad at reconstructing names with less-frequent letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word -> word\n",
      "happy -> happy\n",
      "winter -> winter\n",
      "candle -> candle\n",
      "cherish -> cherish\n"
     ]
    }
   ],
   "source": [
    "# it's decent at some english words that 'sound' like ames:\n",
    "for name in ['word', 'happy', 'winter', 'candle', 'cherish']:\n",
    "    print name, '->', reconstruct(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding -> emerenine\n",
      "automobile -> attoooler\n",
      "air -> arr\n",
      "larynx -> laryne\n"
     ]
    }
   ],
   "source": [
    "# it's worse at more longer, more \"wordy\" names:\n",
    "for name in ['embedding', 'automobile', 'air', 'larynx']:\n",
    "    print name, '->', reconstruct(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ufhoe -> ueooo\n",
      "xyzy -> eyyy\n",
      "ihwrfoecoei -> shereodos\n"
     ]
    }
   ],
   "source": [
    "# predictably, it's terrible at things that aren't even pronouncable strings of letters:\n",
    "for name in ['ufhoe', 'xyzy', 'ihwrfoecoei']:\n",
    "    print name, '->', reconstruct(name)\n",
    "# it even seems to try to turn some into slightly more name-like strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nate : 1.0\n",
      "july : 1.0\n",
      "fridge : 0.833333333333\n",
      "gienigoe : 0.625\n",
      "chzsiucf : 0.375\n",
      "xyxyzzy : 0.0\n"
     ]
    }
   ],
   "source": [
    "# so reconstruction quality seems like a pretty good of how \"name-ish\" a word is\n",
    "# want to give your kid a cool, ~original~ name no one has, but that sounds good?\n",
    "# what good english words sound like names, but aren't?\n",
    "\n",
    "# first, let's build a 'nameliness' score:\n",
    "def nameliness(word):\n",
    "    r = reconstruct(word)\n",
    "    return sum([1 if a == b else 0 for a, b in zip(word, r)]) / float(len(word))\n",
    "\n",
    "for name in ['nate', 'july', 'fridge', 'gienigoe', 'chzsiucf', 'xyxyzzy']:\n",
    "    print name, ':', nameliness(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9268\n",
      "['the', 'and', 'to', 'a', 'in', 'for', 'on', 'that', 'this', 'i', 'not', 'or', 'are', 'at', 'all', 'more', 'an', 'was', 'we', 'can', 'us', 'has', 'free', 'one', 'other', 'do', 'no', 'time', 'they', 'site', 'he', 'any', 'there', 'so', 'get', 'e', 'am', 'were', 's', 'these', 'than', 'find', 'date', 'had', 'list', 'name', 'just', 'state', 'day', 'n', 'world', 're', 'go', 'b', 'last', 'most', 'buy', 'them', 'her', 't', 'add', 'best', 'then', 'good', 'well', 'd', 'm', 'she', 'r', 'many', 'de', 'set', 'mail', 'full', 'games', 'p', 'part', 'center', 'must', 'store', 'made', 'line', 'did', 'those', 'car', 'area', 'want', 'o', 'file', 'both', 'care', 'end', 'him', 'per', 'north', 'posts', 'shop', 'old', 'main', 'call', 'non', 'shall', 'class', 'still', 'money', 'man', 'card', 'jobs', 'food', 'press', 'sale', 'print', 'credit', 'join', 'men', 'sales', 'note', 'gallery', 'table', 'start', 'model', 'cost', 'better', 'july', 'come', 'cart', 'san', 'standard', 'less', 'got', 'let', 'stores', 'side', 'story', 'sell', 'key', 'body', 'paper', 'road', 'ca', 'hard', 'pay', 'four', 'status', 'seller', 'war', 'files', 'china', 'hand', 'sun', 'london', 'id', 'porn', 'share', 'garden', 'added', 'baby', 'net', 'try', 'god', 'head', 'radio', 'cell', 'sure', 'cars', 'tell', 'green', 'close', 'gold', 'feb', 'sep', 'short', 'arts', 'lot', 'daily', 'past', 'due', 'mar', 'land', 'done', 'pro', 'st', 'costs', 'url', 'parts', 'early', 'either', 'word', 'bad', 'edit', 'fast', 'far', 'en', 'feel', 'jul', 'error', 'jun', 'girl', 'domain', 'chapter', 'loan', 'wide', 'india', 'sort', 'none', 'fire', 'chat', 'loss', 'brand', 'bit', 'base', 'stay', 'turn', 'mean', 'cash', 'ad', 'seen', 'port', 'bar']\n"
     ]
    }
   ],
   "source": [
    "# let's grab the top 10k english words, remove the things that are names, and see which can be reconstructed best:\n",
    "# source: https://github.com/first20hours/google-10000-english\n",
    "\n",
    "top_words = list(word.strip() for word in open('../data/google-10000-english.txt'))\n",
    "top_words = list(word for word in top_words if word not in names)\n",
    "print len(top_words)\n",
    "top_words = top_words[:1000] # this is actually kinda slow, so let's stick with the top 1k\n",
    "nameliness_scores = {word: nameliness(word) for word in top_words}\n",
    "print [w for w in top_words if nameliness_scores[w] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's build a big lookup table of all the names and their embeddings:\n",
    "def make_batches(list, size=128):\n",
    "    batches = []\n",
    "    while len(list):\n",
    "        batches.append(list[:min(len(list), size)])\n",
    "        list = list[len(batches[-1]):]\n",
    "    return batches\n",
    "\n",
    "embeddings = {}\n",
    "\n",
    "for batch in make_batches(list(names)):\n",
    "    feed_dict = {\n",
    "        name_placeholder: np.array([name_to_vec(name) for name in batch]),\n",
    "        z_input: np.zeros((len(batch), Z_SIZE)),\n",
    "        use_z_input: 0\n",
    "    }\n",
    "    output_ = session.run(z_mean, feed_dict=feed_dict)\n",
    "    for name, vec in zip(batch, output_):\n",
    "        embeddings[name] = vec\n",
    "    # print 'processed {}/{}'.format(len(embeddings), len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nate is closest to nate\n",
      "yikes is closest to giles\n",
      "panda is closest to hanna\n",
      "ixzhxzi is closest to desirae\n",
      "justxn is closest to justin\n"
     ]
    }
   ],
   "source": [
    "def embed(name):\n",
    "    feed_dict = {\n",
    "        name_placeholder: np.array([name_to_vec(name)]),\n",
    "        z_input: np.zeros((1, Z_SIZE)),\n",
    "        use_z_input: 0\n",
    "    }\n",
    "    output_ = session.run(z_mean, feed_dict=feed_dict)\n",
    "    return output_[0]\n",
    "\n",
    "def nearest(embedding):\n",
    "    def distance(name):\n",
    "        return np.linalg.norm(embedding - embeddings[name])\n",
    "    return min(embeddings.iterkeys(), key=distance)\n",
    "\n",
    "def unembed(embedding):\n",
    "    feed_dict = {\n",
    "        name_placeholder: np.zeros((1, NAME_MAX_LEN)),\n",
    "        z_input: np.array([embedding]),\n",
    "        use_z_input: 1\n",
    "    }\n",
    "    output_ = session.run(decoded_vecs, feed_dict=feed_dict)\n",
    "    return vec_to_name(output_[0])\n",
    "\n",
    "assert unembed(embed('nate')) == 'nate'\n",
    "\n",
    "# print nearest(embed('nate'))\n",
    "for name in ['nate', 'yikes', 'panda', 'ixzhxzi', 'justxn']:\n",
    "    print name, 'is closest to', nearest(embed(name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amy\n",
      "amy\n",
      "ammy\n",
      "ammy\n",
      "aarey\n",
      "nareey\n",
      "nariee\n",
      "faanies\n",
      "franiiss\n",
      "fransisoo\n",
      "fransisoo\n"
     ]
    }
   ],
   "source": [
    "# what happens if we try to interpolate names?\n",
    "def blend_names(name1, name2):\n",
    "    e1 = embed(name1)\n",
    "    e2 = embed(name2)\n",
    "    for i in range(11):\n",
    "        blend = i / 10.0\n",
    "        print unembed(e1 * (1 - blend) + e2 * blend)\n",
    "\n",
    "blend_names('amy', 'francisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nathaniel\n",
      "nathaniel\n",
      "nathaniel\n",
      "kathnnie\n",
      "cathnnee\n",
      "cathnee\n",
      "hathney\n",
      "phanne\n",
      "chene\n",
      "chene\n",
      "chen\n"
     ]
    }
   ],
   "source": [
    "blend_names('nathaniel', 'chen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will\n",
      "will\n",
      "will\n",
      "wille\n",
      "wille\n",
      "wille\n",
      "willie\n",
      "willie\n",
      "willie\n",
      "willia\n",
      "william\n"
     ]
    }
   ],
   "source": [
    "blend_names('will', 'william')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nathaniel\n",
      "nathaniel\n",
      "nathaniel\n",
      "nathaniel\n",
      "nnthaniil\n",
      "nnthaniie\n",
      "nntaaniia\n",
      "innaaniia\n",
      "iinaaatin\n",
      "linntttnn\n",
      "leinnttnn\n"
     ]
    }
   ],
   "source": [
    "blend_names('nathaniel', 'leinahtan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selia\n",
      "seeera\n"
     ]
    }
   ],
   "source": [
    "print nearest(np.zeros(Z_SIZE))\n",
    "print unembed(np.zeros(Z_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nate * 2 = naty\n",
      "willy * 2 = willy\n",
      "sam * 2 = sam\n",
      "polly * 2 = pool\n",
      "jacob * 2 = jaoo\n"
     ]
    }
   ],
   "source": [
    "# what if we multiply names?:\n",
    "for name in ['nate', 'willy', 'sam', 'polly', 'jacob']:\n",
    "    print name, '* 2 =', unembed(embed(name) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-nancy = hhhctttsss\n",
      "-barry = jsuu\n",
      "-chance = ddhhdddsrs\n",
      "-rachel = hhhtrrrrrr\n",
      "-gloria = sselaa\n"
     ]
    }
   ],
   "source": [
    "# what's the opposite of a name?\n",
    "for name in ['nancy', 'barry', 'chance', 'rachel', 'gloria']:\n",
    "    print '-' + name, '=', unembed(-embed(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta\n",
      "justina\n",
      "josepha\n",
      "nanee\n"
     ]
    }
   ],
   "source": [
    "# can we do addition and subtraction?\n",
    "print unembed(embed('alberta') - embed('albert') + embed('robert'))\n",
    "print unembed(embed('alberta') - embed('albert') + embed('justin'))\n",
    "print unembed(embed('alberta') - embed('albert') + embed('joseph'))\n",
    "\n",
    "print unembed(embed('alberta') - embed('albert') + embed('nate')) # doesn't work so well with names ending in vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fooa\n",
      "rssirrrrrr\n",
      "ssio\n",
      "eettte\n",
      "gimarr\n",
      "dedei\n",
      "hharo\n",
      "keran\n",
      "iicuyyyyyy\n",
      "purrlo\n",
      "goroory\n",
      "nanta\n",
      "karia\n",
      "dalor\n",
      "um\n",
      "ahallioa\n",
      "jerlrili\n",
      "hotron\n",
      "marleaa\n",
      "issi\n",
      "samsooa\n",
      "snnt\n",
      "spuuzzzrro\n",
      "alria\n",
      "fuaro\n",
      "keeaa\n",
      "ju\n",
      "pare\n",
      "calonn\n",
      "asilia\n"
     ]
    }
   ],
   "source": [
    "# let's generate some random names:\n",
    "def generate():\n",
    "    return unembed(np.random.normal(size=Z_SIZE))\n",
    "for _ in range(30):\n",
    "    print generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what if we train a GAN to mimick the latent vectors produced by real names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.06\n",
      "0.07\n",
      "0.08\n",
      "0.09\n",
      "0.1\n",
      "0.11\n",
      "0.12\n",
      "0.13\n",
      "0.14\n",
      "0.15\n",
      "0.16\n",
      "0.17\n",
      "0.18\n",
      "0.19\n",
      "0.2\n",
      "0.21\n",
      "0.22\n",
      "0.23\n",
      "0.24\n",
      "0.25\n",
      "0.26\n",
      "0.27\n",
      "0.28\n",
      "0.29\n",
      "0.3\n",
      "0.31\n",
      "0.32\n",
      "0.33\n",
      "0.34\n",
      "0.35\n",
      "0.36\n",
      "0.37\n",
      "0.38\n",
      "0.39\n",
      "0.4\n",
      "0.41\n",
      "0.42\n",
      "0.43\n",
      "0.44\n",
      "0.45\n",
      "0.46\n",
      "0.47\n",
      "0.48\n",
      "0.49\n",
      "0.5\n",
      "0.51\n",
      "0.52\n",
      "0.53\n",
      "0.54\n",
      "0.55\n",
      "0.56\n",
      "0.57\n",
      "0.58\n",
      "0.59\n",
      "0.6\n",
      "0.61\n",
      "0.62\n",
      "0.63\n",
      "0.64\n",
      "0.65\n",
      "0.66\n",
      "0.67\n",
      "0.68\n",
      "0.69\n",
      "0.7\n",
      "0.71\n",
      "0.72\n",
      "0.73\n",
      "0.74\n",
      "0.75\n",
      "0.76\n",
      "0.77\n",
      "0.78\n",
      "0.79\n",
      "0.8\n",
      "0.81\n",
      "0.82\n",
      "0.83\n",
      "0.84\n",
      "0.85\n",
      "0.86\n",
      "0.87\n",
      "0.88\n",
      "0.89\n",
      "0.9\n",
      "0.91\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0.98\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "def unembed_multi(embeddings):\n",
    "    feed_dict = {\n",
    "        name_placeholder: np.zeros((1, NAME_MAX_LEN)),\n",
    "        z_input: np.array(embeddings),\n",
    "        use_z_input: 1\n",
    "    }\n",
    "    output_ = session.run(decoded_vecs, feed_dict=feed_dict)\n",
    "    return [vec_to_name(op) for op in output_]\n",
    "\n",
    "names = open('names.txt', 'w')\n",
    "for i in xrange(10000):\n",
    "    count = 32\n",
    "    embeddings = [np.random.normal(size=Z_SIZE) for _ in xrange(count)]\n",
    "    for name in unembed_multi(embeddings):\n",
    "        names.write(name + '\\n')\n",
    "    if i % 100 == 0: print i / 10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
