{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm\n",
    "from tensorflow.python.framework import ops\n",
    "import os\n",
    "TRAIN = True\n",
    "INFER = False\n",
    "BATCH_SIZE = 128\n",
    "SAVE = True\n",
    "global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "import urllib, StringIO\n",
    "from PIL import Image\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INFER_URLS = [\n",
    "    ('deer', 'http://gfp.sd.gov/hunting/big-game/images/deer1-01.jpg'),\n",
    "    ('frog', 'http://www.gaylordfunkyfish.com/images/Red-Eye-Tree-Frog-300x244.png'),\n",
    "    ('airplane', 'https://media.licdn.com/mpr/mpr/jc/AAEAAQAAAAAAAAMiAAAAJGVlYTU5Y2YyLWQwMzYtNDlmZS04MDdlLWI0ZjJjZWRhYjk4ZQ.jpg'),\n",
    "    ('ship', 'http://www.vships.com/media/92241/passenger-vessel.jpg'),\n",
    "    ('dog', 'https://images-na.ssl-images-amazon.com/images/G/01/img15/pet-products/small-tiles/23695_pets_vertical_store_dogs_small_tile_8._CB312176604_.jpg'),\n",
    "    ('bird', 'http://www.audubon.org/sites/default/files/styles/engagement_card/public/sfw_apa_2013_28342_232388_briankushner_blue_jay_kk_high.jpg')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_decode_single_example(filename, epochs=None):\n",
    "    # first construct a queue containing a list of filenames.\n",
    "    # this lets a user split up there dataset in multiple files to keep\n",
    "    # size down\n",
    "    filename_queue = tf.train.string_input_producer([filename],\n",
    "                                                    num_epochs=epochs)\n",
    "    # Unlike the TFRecordWriter, the TFRecordReader is symbolic\n",
    "    reader = tf.TFRecordReader()\n",
    "    # One can read a single serialized example from a filename\n",
    "    # serialized_example is a Tensor of type string.\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    # The serialized example is converted back to actual values.\n",
    "    # One needs to describe the format of the objects to be returned\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            # We know the length of both fields. If not the\n",
    "            # tf.VarLenFeature could be used\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'image': tf.FixedLenFeature([], 'string')\n",
    "        })\n",
    "    # now return the converted data\n",
    "    label = features['label']\n",
    "    image_raw = features['image']\n",
    "    image = tf.decode_raw(image_raw, tf.uint8)\n",
    "    image = tf.reshape(image, [32,32,3])\n",
    "    image = tf.cast(image, tf.float32) / 255\n",
    "    return label, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distort_image(image):\n",
    "    distorted_image = tf.random_crop(image, [24, 24, 3])\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "    distorted_image = tf.image.random_brightness(distorted_image, max_delta=0.2)\n",
    "    distorted_image = tf.image.random_contrast(distorted_image, lower=0.7, upper=1.3)\n",
    "    # distorted_image = tf.image.per_image_whitening(distorted_image) # renamed to per_image_standardization in latest release\n",
    "    distorted_image = tf.clip_by_value(distorted_image, 0, 1)\n",
    "    return distorted_image\n",
    "\n",
    "def simple_crop(image):\n",
    "    return tf.random_crop(image, [24, 24, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import urllib2\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "def load_image(url):\n",
    "    file = StringIO.StringIO(urllib.urlopen(url).read())\n",
    "    img = Image.open(file)\n",
    "    img = img.resize((24,24), PIL.Image.BICUBIC)\n",
    "    img = img.convert('RGB')\n",
    "    # plt.imshow(img)\n",
    "    img = (np.array(img.getdata()) / 255.0).reshape((24,24,3))\n",
    "    # drop last dimension (alpha):\n",
    "    return img\n",
    "img = load_image('https://images-na.ssl-images-amazon.com/images/G/01/img15/pet-products/small-tiles/23695_pets_vertical_store_dogs_small_tile_8._CB312176604_.jpg')\n",
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# create image batches:\n",
    "\n",
    "if INFER:\n",
    "    queue1 = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[(24,24,3)])\n",
    "    enq = queue1.enqueue_many(np.array([load_image(url) for _, url in INFER_URLS]))\n",
    "    close_input_q = queue1.close()\n",
    "    images_batch = tf.train.batch([queue1.dequeue()], batch_size=BATCH_SIZE, allow_smaller_final_batch=True)\n",
    "    \n",
    "#     sess = tf.Session()\n",
    "#     tf.train.start_queue_runners(sess=sess)\n",
    "#     sess.run(enq)\n",
    "#     sess.run(close)\n",
    "#     img1 = sess.run(images_batch)\n",
    "    labels_batch = tf.placeholder(tf.int64, [None])\n",
    "else:\n",
    "    filename = \"cifar-train.tfrecords\" if TRAIN else \"cifar-test.tfrecords\"\n",
    "    label, image = read_and_decode_single_example(filename, epochs=(None if TRAIN else 1))\n",
    "    if TRAIN:\n",
    "        images_batch, labels_batch = tf.train.shuffle_batch([distort_image(image), label], batch_size=BATCH_SIZE, capacity=1000, min_after_dequeue=500)\n",
    "    else:\n",
    "        images_batch, labels_batch = tf.train.batch([simple_crop(image), label], batch_size=BATCH_SIZE, allow_smaller_final_batch=True)\n",
    "\n",
    "# print 'ya'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')        \n",
    "\n",
    "def weight_var(shape, stddev=0.1, weight_decay=0):\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    v = tf.Variable(initial)\n",
    "    if weight_decay > 0:\n",
    "        l2 = tf.nn.l2_loss(v) * weight_decay\n",
    "        tf.add_to_collection('losses', l2)\n",
    "    return v\n",
    "\n",
    "def create_fc(input, out_size):\n",
    "    # input_dropped = tf.nn.dropout(input, dropout_keep_prob)\n",
    "    in_size = input.get_shape()[-1].value\n",
    "    w = weight_var([in_size, out_size], weight_decay=0.004)\n",
    "    b = weight_var([out_size], weight_decay=0.004)\n",
    "    x = tf.matmul(input, w)\n",
    "    return tf.nn.relu(x + b)\n",
    "\n",
    "def create_conv(input, out_channels, patch_size=3, stride=1, batch_norm=False, dropout=False):\n",
    "    in_channels = input.get_shape()[-1].value\n",
    "    w = weight_var([patch_size, patch_size, in_channels, out_channels])\n",
    "    b = weight_var([out_channels], stddev=0)\n",
    "    conv = tf.nn.conv2d(input, w, strides=[1,stride,stride,1], padding='SAME')\n",
    "    if batch_norm: conv = create_batch_norm(conv)\n",
    "    activation = tf.nn.relu(conv + b)\n",
    "    if dropout: activation = create_dropout(activation)\n",
    "    return activation\n",
    "\n",
    "def create_max_pool(inputs, ksize=2, stride=2):\n",
    "    return tf.nn.max_pool(inputs, ksize=[1, ksize, ksize, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def create_batch_norm(inputs):\n",
    "    return batch_norm(inputs, is_training=TRAIN)\n",
    "\n",
    "def create_dropout(inputs):\n",
    "    return tf.nn.dropout(inputs, dropout_keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(images):\n",
    "    # images are 24x24x3\n",
    "    for size in [32, 64, 64]:\n",
    "        images = create_conv(images, size, dropout=True, batch_norm=False)\n",
    "        images = create_max_pool(images)\n",
    "    # now images are 6x6x32 = 1176:\n",
    "    n_channels = images.get_shape()[-1].value\n",
    "    vecs = tf.reshape(images, [-1, 3*3*n_channels])\n",
    "    # vecs = tf.reshape(images, [-1, 24*24*3])\n",
    "    vecs = create_fc(vecs, 512)\n",
    "    vecs = batch_norm(vecs)\n",
    "    logits = create_fc(vecs, 10)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS_PER_DECAY = 300.0      # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "INITIAL_LEARNING_RATE = 0.001       # Initial learning rate.\n",
    "EXAMPLES_PER_EPOCH = 60000\n",
    "\n",
    "learn_rate = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                  global_step,\n",
    "                                  EXAMPLES_PER_EPOCH * NUM_EPOCHS_PER_DECAY / BATCH_SIZE,\n",
    "                                  LEARNING_RATE_DECAY_FACTOR,\n",
    "                                  staircase=False)\n",
    "\n",
    "# create optimizer:\n",
    "logits = forward(images_batch)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels_batch))\n",
    "loss = cross_entropy # + tf.add_n(tf.get_collection('losses'))\n",
    "optimizer = tf.train.AdamOptimizer(learn_rate)\n",
    "train_step = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "probs = tf.nn.softmax(logits)\n",
    "predictions = tf.argmax(probs, 1)\n",
    "n_correct = tf.reduce_sum(tf.cast(tf.equal(predictions, labels_batch), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not restore from checkpoint\n"
     ]
    }
   ],
   "source": [
    "# 16 is a good model\n",
    "save_path = 'models/1'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "session = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "session.run(init_op)\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "ckpt = tf.train.get_checkpoint_state(save_path)\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    print 'Restored from checkpoint', ckpt.model_checkpoint_path\n",
    "else:\n",
    "    print 'Did not restore from checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1; Loss: 2.81876397133; learn rate: 0.0010000000475\n",
      "Step: 51; Loss: 2.27156567574; learn rate: 0.000999181647785\n",
      "Step: 101; Loss: 1.91344153881; learn rate: 0.000998363946564\n",
      "saved\n",
      "[0 3 6 7 6 2 9 6 6 3 4 9 9 6 8 6 4 8 9 6 4 8 6 8 5 8 9 4 6 4 3 8 6 5 3 0 9\n",
      " 7 3 8 8 6 5 7 5 5 8 1 3 6 8 6 3 4 6 5 7 0 7 9 1 8 3 0 8 9 1 7 8 0 5 8 8 0\n",
      " 6 5 2 3 8 4 8 3 8 8 1 8 7 9 7 3 6 7 3 1 9 9 3 4 5 4 7 9 7 8 5 7 3 3 1 3 4\n",
      " 1 4 5 5 4 0 5 0 3 3 8 1 8 4 0 3 8]\n",
      "Step: 151; Loss: 1.90413033962; learn rate: 0.000997546943836\n",
      "Step: 201; Loss: 1.80249452591; learn rate: 0.000996730523184\n",
      "Step: 251; Loss: 1.79190135002; learn rate: 0.000995914917439\n",
      "saved\n",
      "[3 3 9 2 6 3 6 8 2 9 8 7 4 3 3 2 7 8 5 8 2 2 7 2 6 8 2 1 6 9 3 1 8 7 1 6 7\n",
      " 6 4 1 7 7 9 3 9 5 3 2 7 8 4 8 1 8 1 9 1 1 8 5 4 6 8 1 3 2 3 2 5 2 7 2 4 2\n",
      " 6 7 8 8 9 9 1 8 3 8 8 0 5 1 8 8 8 1 0 7 2 7 8 6 9 3 7 5 2 7 6 3 5 1 5 0 8\n",
      " 6 5 5 8 4 8 6 0 5 0 0 5 8 7 6 5 1]\n",
      "Step: 301; Loss: 1.61766886711; learn rate: 0.000995099893771\n",
      "Step: 351; Loss: 1.76752400398; learn rate: 0.000994285568595\n",
      "Step: 401; Loss: 1.4927008152; learn rate: 0.000993471825495\n",
      "saved\n",
      "[0 7 3 3 6 8 4 9 3 3 7 8 6 0 6 9 8 8 1 1 8 7 1 7 5 4 3 0 3 9 5 1 2 8 4 1 1\n",
      " 5 3 8 9 5 4 9 7 5 4 2 1 4 6 6 9 5 9 3 3 1 3 7 5 9 4 3 2 3 7 0 9 6 4 9 6 4\n",
      " 8 3 3 9 2 7 4 4 5 6 3 8 0 6 1 9 0 8 7 6 0 2 1 2 7 9 3 5 4 3 0 8 7 4 8 8 9\n",
      " 1 7 2 8 6 8 7 5 2 4 6 6 9 7 0 1 1]\n",
      "Step: 451; Loss: 1.577891469; learn rate: 0.000992658780888\n",
      "Step: 501; Loss: 1.70180416107; learn rate: 0.000991846434772\n",
      "Step: 551; Loss: 1.52057445049; learn rate: 0.000991034787148\n",
      "saved\n",
      "[3 3 1 1 9 7 2 5 7 1 9 6 3 8 8 2 0 2 4 6 5 5 9 0 3 2 4 9 7 7 0 8 2 0 2 6 2\n",
      " 1 6 5 9 9 1 3 6 6 1 5 6 9 6 6 4 5 2 6 8 8 3 6 7 1 7 2 5 0 9 9 5 7 6 9 9 4\n",
      " 5 0 4 7 5 7 6 6 0 1 0 6 6 9 7 6 6 3 7 3 8 1 8 7 6 7 9 6 8 7 8 5 0 4 6 9 8\n",
      " 3 1 8 9 2 9 6 7 0 6 6 4 6 8 7 6 7]\n",
      "Step: 601; Loss: 1.52149176598; learn rate: 0.000990223721601\n",
      "Step: 651; Loss: 1.5703099966; learn rate: 0.000989413470961\n",
      "Step: 701; Loss: 1.38717603683; learn rate: 0.000988603685983\n",
      "saved\n",
      "[7 9 6 9 0 9 8 8 6 7 0 0 3 9 2 3 0 3 0 1 5 9 6 7 5 3 9 5 6 8 8 2 4 6 7 9 7\n",
      " 8 2 9 0 7 6 4 8 1 3 9 9 4 5 9 0 4 4 6 4 3 4 2 2 2 4 3 1 9 7 4 6 7 9 4 5 7\n",
      " 2 5 0 8 0 4 7 2 0 9 1 8 8 0 3 1 5 5 2 7 9 6 9 6 6 3 5 8 1 4 4 2 9 1 9 5 7\n",
      " 1 2 4 8 8 9 0 6 5 8 9 6 4 4 5 2 8]\n",
      "Step: 751; Loss: 1.46471071243; learn rate: 0.000987794715911\n",
      "Step: 801; Loss: 1.49185609818; learn rate: 0.000986986327916\n",
      "Step: 851; Loss: 1.35987305641; learn rate: 0.000986178638414\n",
      "saved\n",
      "[1 3 7 0 7 5 7 4 7 6 9 1 9 1 4 8 9 1 9 9 2 1 8 6 2 5 6 3 0 0 0 0 3 6 8 0 3\n",
      " 8 6 1 8 0 9 0 4 3 9 3 6 3 4 0 2 2 1 3 8 9 5 1 3 1 0 2 0 2 7 8 6 7 8 3 6 4\n",
      " 9 9 0 2 7 0 9 3 9 4 4 2 9 5 9 9 1 1 1 4 3 8 2 0 0 6 4 3 1 7 0 6 3 7 1 3 3\n",
      " 5 0 1 9 7 8 2 9 2 5 4 8 4 2 5 3 3]\n",
      "Step: 901; Loss: 1.3672683239; learn rate: 0.000985371530987\n",
      "Step: 951; Loss: 1.39928400517; learn rate: 0.000984565122053\n",
      "Step: 1001; Loss: 1.65761256218; learn rate: 0.000983759411611\n",
      "saved\n",
      "[8 7 3 7 4 2 9 4 1 9 9 0 6 2 0 9 0 3 8 8 9 3 4 4 7 1 0 4 7 4 2 8 9 3 8 5 8\n",
      " 4 7 3 0 7 3 7 9 3 5 7 7 1 1 2 6 8 3 8 8 2 2 3 0 8 4 6 9 0 1 7 7 4 3 1 2 9\n",
      " 8 9 9 1 2 3 4 8 0 5 4 7 8 0 4 6 9 0 2 1 2 6 3 8 4 5 0 1 1 8 3 9 6 0 0 3 9\n",
      " 9 7 2 7 1 3 3 2 5 1 2 4 6 2 1 2 3]\n",
      "Step: 1051; Loss: 1.41675460339; learn rate: 0.00098295439966\n",
      "Step: 1101; Loss: 1.40132200718; learn rate: 0.000982149969786\n",
      "Step: 1151; Loss: 1.31525850296; learn rate: 0.000981346121989\n",
      "saved\n",
      "[8 3 6 1 0 4 1 2 7 1 2 6 7 0 9 6 0 5 1 7 9 8 3 7 3 4 5 5 3 5 6 2 6 9 4 6 3\n",
      " 4 2 5 3 7 4 9 2 8 2 5 5 9 8 0 0 1 0 7 8 3 1 6 2 0 4 5 8 0 5 3 7 9 6 6 0 6\n",
      " 3 1 9 1 2 0 2 6 4 0 3 0 1 9 0 3 6 3 7 6 8 7 0 5 8 3 6 5 9 1 6 2 5 9 3 9 3\n",
      " 1 0 9 5 8 7 9 4 0 7 4 7 6 5 2 9 4]\n",
      "Step: 1201; Loss: 1.30468583107; learn rate: 0.000980543089099\n",
      "Step: 1251; Loss: 1.48021507263; learn rate: 0.000979740638286\n",
      "Step: 1301; Loss: 1.1136636734; learn rate: 0.000978938885964\n",
      "saved\n",
      "[4 4 7 0 7 2 8 1 8 2 4 3 0 8 4 7 0 9 5 6 0 6 4 7 8 8 6 7 2 3 6 6 0 4 8 6 8\n",
      " 6 1 7 9 9 9 6 7 8 7 6 9 9 5 5 7 0 0 4 7 8 8 3 6 6 5 2 9 0 8 2 2 0 6 9 3 4\n",
      " 1 2 2 5 1 0 0 3 3 8 4 5 0 6 8 3 3 9 3 7 4 2 8 5 9 8 9 4 0 5 3 5 9 5 4 1 7\n",
      " 7 7 7 6 7 0 4 7 2 4 0 4 3 4 5 8 9]\n",
      "Step: 1351; Loss: 1.33129155636; learn rate: 0.00097813771572\n",
      "Step: 1401; Loss: 1.28312110901; learn rate: 0.000977337243967\n",
      "Step: 1451; Loss: 1.43233060837; learn rate: 0.000976537412498\n",
      "saved\n",
      "[8 8 4 7 8 3 2 2 4 7 5 1 2 7 3 5 1 1 8 9 2 4 2 6 9 9 1 3 5 8 7 1 5 4 1 7 8\n",
      " 9 3 0 1 5 6 9 4 9 2 6 2 0 1 6 3 6 8 4 1 6 5 4 4 4 3 5 0 7 9 0 4 4 4 0 8 8\n",
      " 8 8 4 6 9 4 7 2 2 6 0 0 3 3 4 6 7 6 9 7 6 9 2 7 1 9 0 7 2 1 3 2 6 6 7 7 8\n",
      " 3 2 8 2 9 6 9 7 1 2 3 4 2 9 1 8 1]\n",
      "Step: 1501; Loss: 1.36535835266; learn rate: 0.000975738279521\n",
      "Step: 1551; Loss: 1.23800086975; learn rate: 0.000974939728621\n",
      "Step: 1601; Loss: 1.37964606285; learn rate: 0.000974141876213\n",
      "saved\n",
      "[9 6 2 9 6 7 5 4 6 6 4 8 8 5 5 6 3 0 6 5 7 9 7 1 4 9 9 3 7 2 0 4 6 9 2 8 8\n",
      " 9 7 9 9 8 8 9 8 2 7 4 6 1 5 7 9 5 8 9 7 1 5 6 6 2 8 8 8 1 6 1 0 2 1 1 5 3\n",
      " 8 2 4 4 6 3 0 7 4 1 8 8 5 0 0 5 2 4 5 8 9 4 2 2 0 1 7 3 9 7 4 5 6 7 9 5 0\n",
      " 6 7 8 8 3 9 8 1 9 3 9 6 1 4 3 1 8]\n",
      "Step: 1651; Loss: 1.2334985733; learn rate: 0.000973344664089\n",
      "Step: 1701; Loss: 1.36884641647; learn rate: 0.00097254809225\n",
      "Step: 1751; Loss: 1.28507447243; learn rate: 0.00097175227711\n",
      "saved\n",
      "[0 1 0 8 7 3 6 6 4 2 8 3 6 1 1 3 7 0 4 8 9 8 2 9 1 0 4 1 6 6 7 9 2 6 1 4 8\n",
      " 4 9 7 2 0 1 1 5 5 7 1 9 5 3 2 1 1 1 6 5 6 9 3 7 1 3 0 2 4 3 8 6 4 3 4 1 0\n",
      " 5 7 0 0 7 1 9 7 7 3 6 4 3 7 3 4 7 8 9 3 7 4 9 1 9 6 1 7 7 0 6 6 8 0 9 3 3\n",
      " 0 3 9 9 3 7 3 4 5 6 0 9 2 6 8 8 4]\n",
      "Step: 1801; Loss: 1.08134651184; learn rate: 0.000970957044046\n",
      "Step: 1851; Loss: 1.18655729294; learn rate: 0.000970162451267\n",
      "Step: 1901; Loss: 1.23379313946; learn rate: 0.000969368498772\n",
      "saved\n",
      "[0 0 4 9 2 1 2 9 0 4 6 2 8 4 1 7 3 5 0 7 7 6 9 3 1 9 1 0 8 1 7 0 9 3 7 4 5\n",
      " 1 4 3 2 8 4 8 9 8 3 4 8 0 1 7 3 6 5 4 0 1 7 4 6 6 3 8 5 7 9 1 3 4 1 3 1 4\n",
      " 8 6 7 3 2 4 5 4 9 5 3 4 1 2 8 0 0 8 7 1 1 9 7 5 4 4 6 6 6 6 4 7 3 9 1 9 1\n",
      " 6 6 4 6 6 9 7 1 3 5 7 3 2 5 5 1 4]\n",
      "Step: 1951; Loss: 1.30763888359; learn rate: 0.000968575244769\n",
      "Step: 2001; Loss: 1.2082118988; learn rate: 0.000967782514635\n",
      "Step: 2051; Loss: 1.35538721085; learn rate: 0.000966990599409\n",
      "saved\n",
      "[3 7 9 2 3 5 7 6 6 9 2 0 4 4 6 2 7 0 2 8 3 1 3 2 8 8 4 3 0 8 9 3 2 2 3 6 8\n",
      " 0 6 1 7 4 1 3 0 4 6 7 0 2 6 8 9 0 3 1 2 4 0 5 2 8 9 7 0 0 5 9 1 0 5 3 9 4\n",
      " 5 8 7 6 8 6 8 0 9 5 4 3 3 1 8 3 4 0 2 9 0 2 8 9 9 9 2 4 5 7 1 9 4 9 1 6 0\n",
      " 1 6 6 7 3 4 5 7 8 4 6 9 2 1 7 8 7]\n",
      "Step: 2101; Loss: 1.27304911613; learn rate: 0.000966199208051\n",
      "Step: 2151; Loss: 1.31185889244; learn rate: 0.000965408573393\n",
      "Step: 2201; Loss: 1.37286198139; learn rate: 0.000964618462604\n",
      "saved\n",
      "[7 4 7 2 3 0 6 7 1 4 3 7 1 6 4 7 1 9 7 0 7 7 0 5 7 2 4 1 1 8 7 1 9 6 2 5 6\n",
      " 4 6 0 4 3 3 2 4 6 7 1 3 9 1 6 8 9 6 3 1 6 9 9 8 3 7 8 9 5 8 3 6 2 6 4 3 7\n",
      " 6 9 5 6 3 3 6 0 2 0 6 1 2 2 8 8 1 9 0 5 0 9 3 6 9 8 2 0 2 3 3 8 3 2 2 5 8\n",
      " 9 5 5 1 6 2 5 5 0 0 2 2 2 8 2 6 1]\n",
      "Step: 2251; Loss: 1.31536471844; learn rate: 0.000963829108514\n",
      "Step: 2301; Loss: 1.41171336174; learn rate: 0.000963040336501\n",
      "Step: 2351; Loss: 1.33588659763; learn rate: 0.000962252204772\n",
      "saved\n",
      "[3 9 8 1 6 9 8 2 0 2 4 8 5 9 3 4 0 7 3 9 1 8 2 4 5 8 0 5 4 6 1 8 6 5 7 4 4\n",
      " 3 4 6 0 8 8 4 5 6 6 8 8 5 5 3 7 3 6 2 5 9 7 6 8 8 0 8 3 2 3 5 6 3 8 4 5 2\n",
      " 7 6 5 4 7 0 3 8 9 3 8 1 5 1 3 0 6 4 6 3 9 4 7 4 9 1 4 9 8 1 9 1 6 0 7 7 0\n",
      " 3 3 1 1 0 6 5 1 6 8 4 2 7 5 7 1 4]\n",
      "Step: 2401; Loss: 1.19615781307; learn rate: 0.000961464771535\n",
      "Step: 2451; Loss: 1.21229588985; learn rate: 0.000960677920375\n",
      "Step: 2501; Loss: 1.21175825596; learn rate: 0.000959891709499\n",
      "saved\n",
      "[5 4 8 2 6 0 0 4 9 9 7 5 1 8 2 4 1 2 0 5 8 1 1 0 4 2 7 9 6 6 9 6 0 9 5 6 2\n",
      " 0 4 1 7 9 4 3 3 1 8 2 6 0 0 3 3 0 0 8 5 8 5 0 1 4 2 2 5 4 6 2 4 7 9 8 4 7\n",
      " 2 5 4 0 4 3 7 0 7 0 2 1 4 0 6 5 8 1 4 9 1 1 7 3 1 5 7 1 1 0 6 7 2 7 1 3 3\n",
      " 4 9 5 9 3 4 4 7 3 1 4 9 4 4 9 2 9]\n",
      "Step: 2551; Loss: 1.19717800617; learn rate: 0.000959106197115\n",
      "Step: 2601; Loss: 1.06551337242; learn rate: 0.000958321325015\n",
      "Step: 2651; Loss: 1.02621352673; learn rate: 0.000957537034992\n",
      "saved\n",
      "[7 1 9 6 4 9 8 6 3 2 7 6 3 7 8 3 7 6 9 9 4 2 9 7 8 4 0 3 9 0 1 2 9 2 4 2 0\n",
      " 4 0 6 9 7 8 9 7 7 7 9 2 8 3 0 3 8 4 4 4 1 9 5 0 1 8 9 4 1 6 5 9 3 6 3 4 5\n",
      " 5 7 1 5 8 5 2 5 5 5 2 0 3 5 7 4 9 2 9 0 9 0 4 1 0 7 1 3 1 5 5 5 1 1 4 3 5\n",
      " 9 1 7 8 6 4 6 9 7 0 6 4 0 8 5 6 1]\n",
      "Step: 2701; Loss: 0.954281926155; learn rate: 0.000956753385253\n",
      "Step: 2751; Loss: 1.07041370869; learn rate: 0.000955970375799\n",
      "Step: 2801; Loss: 1.23741734028; learn rate: 0.000955188064836\n",
      "saved\n",
      "[2 2 0 2 1 1 4 7 7 8 8 9 5 5 2 5 4 1 9 7 5 7 4 6 5 6 5 7 3 1 3 6 3 0 0 8 7\n",
      " 0 7 8 8 6 8 6 7 7 6 8 2 7 1 9 5 8 8 8 0 1 8 9 2 0 6 3 9 1 4 6 6 5 7 7 8 8\n",
      " 7 5 7 7 9 8 3 8 7 4 4 4 6 8 2 3 9 2 1 4 4 6 6 9 6 4 7 1 9 1 0 0 9 4 1 0 9\n",
      " 6 6 0 4 1 5 4 3 4 4 2 6 8 5 7 3 5]\n",
      "Step: 2851; Loss: 1.09189128876; learn rate: 0.000954406394158\n",
      "Step: 2901; Loss: 1.18055319786; learn rate: 0.000953625305556\n",
      "Step: 2951; Loss: 1.25306546688; learn rate: 0.000952844915446\n",
      "saved\n",
      "[4 3 5 4 7 1 3 8 3 1 6 6 2 0 8 5 8 1 1 4 5 0 2 2 9 7 8 3 0 5 7 1 2 0 4 9 7\n",
      " 3 3 7 1 7 2 7 0 9 7 4 6 7 2 9 7 1 5 6 8 3 0 7 3 7 7 0 4 7 6 1 0 7 6 6 8 1\n",
      " 3 3 7 8 5 3 6 3 0 1 5 4 2 6 8 2 8 3 6 1 6 3 7 3 9 8 8 5 7 3 9 1 4 2 1 0 2\n",
      " 0 0 4 6 7 6 2 9 1 6 3 2 9 8 2 4 8]\n",
      "Step: 3001; Loss: 1.37678742409; learn rate: 0.00095206516562\n",
      "Step: 3051; Loss: 1.09540915489; learn rate: 0.000951285997871\n",
      "Step: 3101; Loss: 1.04759156704; learn rate: 0.000950507528614\n",
      "saved\n",
      "[2 5 9 2 9 6 7 6 1 9 0 2 7 2 2 1 6 6 5 0 8 6 0 5 0 2 0 2 9 0 6 3 9 7 1 1 2\n",
      " 4 1 1 8 3 6 7 8 9 3 5 8 2 8 3 7 8 7 6 1 9 7 8 8 9 8 2 4 4 6 0 2 4 4 5 5 2\n",
      " 2 9 5 5 4 8 8 2 4 1 2 5 8 4 6 9 3 1 3 9 8 2 5 7 0 5 4 1 5 9 9 1 4 6 9 8 8\n",
      " 9 5 3 2 0 2 4 2 1 9 3 9 7 1 4 3 0]\n",
      "Step: 3151; Loss: 1.29987549782; learn rate: 0.000949729641434\n",
      "Step: 3201; Loss: 1.13399755955; learn rate: 0.000948952394538\n",
      "Step: 3251; Loss: 1.12288880348; learn rate: 0.000948175846133\n",
      "saved\n",
      "[7 4 0 5 1 3 6 1 7 6 1 8 1 0 8 8 3 2 6 4 9 9 6 1 9 3 9 7 0 4 1 9 2 7 6 9 9\n",
      " 5 7 2 8 4 3 5 4 1 5 5 3 0 8 8 3 4 3 2 8 2 8 0 5 6 5 3 2 6 9 5 9 6 9 3 6 9\n",
      " 9 6 6 0 8 6 4 0 3 7 7 5 5 3 8 4 2 1 9 0 8 4 6 5 4 7 8 2 5 2 9 4 4 3 6 1 6\n",
      " 0 2 1 0 7 5 8 1 2 3 0 5 4 8 4 1 6]\n",
      "Step: 3301; Loss: 1.01784348488; learn rate: 0.000947399879806\n",
      "Step: 3351; Loss: 1.03681111336; learn rate: 0.000946624553762\n",
      "Step: 3401; Loss: 0.866377592087; learn rate: 0.000945849926211\n",
      "saved\n",
      "[6 6 1 9 7 1 8 5 9 1 3 7 8 8 1 4 5 2 7 7 1 0 3 0 7 8 3 6 5 3 0 8 8 0 6 0 9\n",
      " 3 0 4 6 3 5 5 9 6 0 1 2 8 3 0 1 9 5 0 8 5 9 0 1 2 2 4 5 8 9 5 2 3 9 3 6 5\n",
      " 5 1 4 2 9 3 1 5 5 2 9 5 6 3 0 1 5 4 5 0 4 4 3 2 0 2 8 9 3 1 8 7 5 0 1 8 4\n",
      " 6 5 2 0 7 6 3 8 9 0 1 9 7 7 8 6 9]\n",
      "Step: 3451; Loss: 1.07234764099; learn rate: 0.000945075938944\n",
      "Step: 3501; Loss: 1.08911168575; learn rate: 0.000944302475546\n",
      "Step: 3551; Loss: 1.18414652348; learn rate: 0.000943529710639\n",
      "saved\n",
      "[6 0 2 0 6 9 3 8 9 2 1 4 9 1 1 8 7 3 0 1 4 8 9 8 9 8 6 8 6 7 0 5 8 6 6 6 4\n",
      " 5 6 9 8 4 8 7 4 4 5 8 7 7 3 0 4 4 5 1 9 2 1 1 4 7 0 5 3 2 9 5 7 4 1 2 4 8\n",
      " 3 6 7 9 8 4 6 1 9 1 7 8 2 2 3 4 5 9 5 7 5 6 1 9 5 8 6 5 7 9 5 4 4 3 1 9 4\n",
      " 0 0 0 6 7 3 9 8 5 6 1 1 0 8 8 8 2]\n",
      "Step: 3601; Loss: 1.24483942986; learn rate: 0.000942757586017\n",
      "Step: 3651; Loss: 1.0827767849; learn rate: 0.000941986043472\n",
      "Step: 3701; Loss: 1.39206147194; learn rate: 0.000941215141211\n",
      "saved\n",
      "[9 1 0 7 2 2 0 1 6 9 4 6 7 4 8 3 3 3 3 6 6 7 6 3 1 5 3 7 6 0 0 2 3 0 4 1 1\n",
      " 0 3 1 8 6 9 6 3 2 8 9 5 3 8 3 7 2 4 9 2 8 8 9 0 0 2 8 4 6 9 1 7 7 5 9 7 1\n",
      " 8 5 1 9 3 4 7 4 3 1 6 9 6 7 4 7 3 0 5 1 2 6 2 0 7 2 7 9 3 4 4 0 9 5 2 0 6\n",
      " 9 7 5 0 8 0 1 7 8 1 9 6 4 1 3 9 7]\n",
      "Step: 3751; Loss: 1.23582470417; learn rate: 0.000940444937441\n",
      "Step: 3801; Loss: 1.27138912678; learn rate: 0.000939675315749\n",
      "Step: 3851; Loss: 1.09902346134; learn rate: 0.000938906276133\n",
      "saved\n",
      "[5 9 8 2 3 3 2 4 9 0 4 6 5 6 7 4 2 5 1 7 9 1 1 1 0 0 7 2 2 2 0 5 9 7 2 0 3\n",
      " 1 6 3 0 5 8 4 7 8 0 4 8 2 1 1 2 8 8 4 9 2 7 8 4 6 5 7 7 9 1 7 9 3 7 0 2 3\n",
      " 7 9 6 1 4 8 2 6 8 3 9 5 4 7 9 1 0 6 3 4 5 9 4 7 5 3 2 6 7 8 4 9 8 6 1 5 9\n",
      " 7 0 6 6 3 7 5 5 4 6 5 6 5 8 5 6 2]\n",
      "Step: 3901; Loss: 1.20525598526; learn rate: 0.000938137935009\n",
      "Step: 3951; Loss: 1.04816877842; learn rate: 0.000937370234169\n",
      "Step: 4001; Loss: 1.07111167908; learn rate: 0.000936603057198\n",
      "saved\n",
      "[2 6 2 4 1 2 0 9 0 9 2 6 1 6 7 2 7 3 1 2 6 8 2 7 5 9 4 5 3 0 6 1 8 4 0 4 9\n",
      " 1 7 5 6 6 9 2 2 7 9 8 6 8 1 7 4 7 9 9 8 3 6 2 0 6 0 3 5 0 8 6 5 0 4 6 6 0\n",
      " 6 9 6 5 8 8 7 6 8 5 5 6 2 3 9 2 5 2 6 5 3 3 6 4 5 7 8 2 6 5 9 0 0 2 1 6 1\n",
      " 8 9 8 5 2 9 3 0 3 8 8 7 4 4 4 8 1]\n",
      "Step: 4051; Loss: 1.04920530319; learn rate: 0.000935836578719\n",
      "Step: 4101; Loss: 0.825106859207; learn rate: 0.000935070682317\n",
      "Step: 4151; Loss: 1.19705653191; learn rate: 0.000934305484407\n",
      "saved\n",
      "[3 4 0 8 7 0 3 2 9 6 8 1 9 1 5 4 8 6 3 8 3 7 3 8 8 3 2 0 7 1 6 0 3 7 3 6 9\n",
      " 0 4 1 6 3 1 9 2 2 4 5 6 1 4 8 1 0 1 9 2 7 0 1 3 7 3 1 2 8 6 7 7 3 3 5 0 1\n",
      " 3 3 2 7 3 1 0 3 6 7 9 1 5 4 0 1 3 7 2 3 6 4 1 2 3 9 8 7 3 9 8 9 1 9 1 4 2\n",
      " 8 2 2 8 4 4 7 4 5 8 1 2 1 3 5 9 0]\n",
      "Step: 4201; Loss: 1.06257164478; learn rate: 0.000933540868573\n",
      "Step: 4251; Loss: 1.18700480461; learn rate: 0.000932776893023\n",
      "Step: 4301; Loss: 1.0458855629; learn rate: 0.000932013557758\n",
      "saved\n",
      "[4 7 0 5 5 9 4 3 2 2 2 3 0 3 5 7 9 0 7 5 8 0 3 2 6 6 9 7 6 0 2 4 0 6 9 7 8\n",
      " 9 5 6 0 9 7 7 6 1 2 1 0 5 4 5 5 3 6 1 6 7 3 7 3 7 8 7 7 6 2 3 1 9 0 9 4 1\n",
      " 3 0 5 2 9 0 8 8 9 5 6 6 0 5 8 7 7 0 8 6 7 6 8 9 5 4 1 8 0 5 8 5 5 4 3 8 4\n",
      " 6 8 0 3 6 1 4 1 0 5 0 7 9 6 3 2 0]\n",
      "Step: 4351; Loss: 1.02162194252; learn rate: 0.00093125080457\n",
      "Step: 4401; Loss: 1.26688313484; learn rate: 0.000930488691665\n",
      "Step: 4451; Loss: 0.911040902138; learn rate: 0.000929727219045\n",
      "saved\n",
      "[2 2 4 8 4 9 9 7 2 0 7 5 4 6 7 0 8 1 2 4 9 6 4 8 9 5 7 8 7 6 8 2 7 4 6 3 5\n",
      " 6 4 0 2 3 3 9 3 6 1 3 9 4 6 2 2 9 1 0 8 2 7 1 9 4 1 6 2 3 0 1 7 4 8 8 7 0\n",
      " 7 0 2 7 9 8 1 5 2 4 4 6 3 2 3 8 5 1 1 2 5 1 0 4 2 2 4 5 9 7 6 8 6 1 9 9 4\n",
      " 1 3 4 1 0 0 0 0 1 7 7 8 0 3 3 0 8]\n",
      "Step: 4501; Loss: 1.07641685009; learn rate: 0.000928966386709\n",
      "Step: 4551; Loss: 0.954661428928; learn rate: 0.000928206194658\n",
      "Step: 4601; Loss: 0.803371846676; learn rate: 0.000927446584683\n",
      "saved\n",
      "[9 1 1 5 1 2 6 9 9 7 2 0 0 2 8 2 3 4 2 7 3 7 9 8 7 9 5 8 6 0 9 7 1 9 0 7 5\n",
      " 9 3 9 0 7 2 7 5 4 6 1 6 7 7 6 5 0 5 1 6 9 0 7 0 6 4 8 8 8 6 8 0 5 2 9 8 7\n",
      " 9 3 1 5 3 5 8 5 8 0 3 1 0 1 7 6 3 8 1 3 0 2 0 2 2 4 8 3 2 7 9 5 5 8 0 1 6\n",
      " 6 0 6 1 9 9 5 8 1 5 4 1 4 6 9 9 4]\n",
      "Step: 4651; Loss: 1.17773580551; learn rate: 0.000926687556785\n",
      "Step: 4701; Loss: 1.0800344944; learn rate: 0.000925929227378\n",
      "Step: 4751; Loss: 0.967194974422; learn rate: 0.000925171480048\n",
      "saved\n",
      "[5 3 8 5 1 3 7 5 2 6 5 2 8 5 8 1 4 2 1 2 0 1 0 4 9 9 5 7 3 3 8 4 8 0 8 9 7\n",
      " 2 4 9 9 9 7 1 0 7 7 2 6 0 8 6 3 3 7 5 4 1 2 6 6 0 5 7 6 1 5 0 4 4 3 3 6 9\n",
      " 6 3 7 5 5 0 3 6 5 4 9 2 0 4 4 9 9 5 9 7 9 4 7 1 4 7 8 8 7 4 8 9 1 1 1 1 8\n",
      " 2 5 7 5 8 4 1 5 4 1 9 5 5 5 9 6 5]\n",
      "Step: 4801; Loss: 1.04497027397; learn rate: 0.000924414314795\n",
      "Step: 4851; Loss: 0.89896261692; learn rate: 0.000923657789826\n",
      "Step: 4901; Loss: 1.1083573103; learn rate: 0.000922901963349\n",
      "saved\n",
      "[3 2 4 7 9 8 0 5 1 4 1 8 6 9 1 5 9 0 6 3 2 3 6 7 6 8 7 0 3 7 2 3 3 9 2 6 4\n",
      " 4 9 5 0 2 5 8 6 8 4 5 1 3 6 8 9 5 3 7 1 7 4 9 5 7 1 7 5 4 9 6 7 6 7 8 6 6\n",
      " 0 3 9 9 0 1 2 8 4 8 6 5 7 4 6 1 3 9 0 1 3 6 2 7 8 1 9 5 8 5 1 6 1 5 4 3 9\n",
      " 6 6 4 4 5 3 8 9 1 5 6 3 5 5 4 5 1]\n",
      "Step: 4951; Loss: 1.03868353367; learn rate: 0.000922146718949\n",
      "Step: 5001; Loss: 1.12965536118; learn rate: 0.000921392056625\n",
      "Step: 5051; Loss: 1.04718995094; learn rate: 0.000920638034586\n",
      "saved\n",
      "[7 3 3 5 6 4 3 7 7 7 8 8 2 5 2 0 2 6 9 6 6 7 6 4 2 7 8 3 1 2 6 5 3 5 0 2 9\n",
      " 1 5 2 3 7 8 6 4 1 3 3 3 7 8 4 9 4 2 5 5 8 6 4 9 3 1 0 0 4 2 8 4 0 5 5 4 3\n",
      " 1 2 9 6 2 1 1 8 5 9 4 2 9 0 7 6 6 1 1 1 7 0 8 0 5 0 7 5 7 1 8 9 7 3 3 8 2\n",
      " 8 7 8 7 9 2 5 6 8 3 0 6 8 4 3 8 1]\n",
      "Step: 5101; Loss: 0.904441535473; learn rate: 0.000919884594623\n",
      "Step: 5151; Loss: 0.936151862144; learn rate: 0.000919131794944\n",
      "Step: 5201; Loss: 1.01829469204; learn rate: 0.00091837963555\n",
      "saved\n",
      "[5 6 6 1 0 1 9 3 1 8 9 7 4 3 4 6 9 2 3 0 6 9 3 2 9 5 7 5 2 2 0 5 8 5 9 4 6\n",
      " 8 5 9 6 8 5 6 8 1 1 0 1 4 5 9 4 7 7 9 3 8 0 4 7 1 6 9 0 1 1 6 1 0 5 2 6 7\n",
      " 0 7 9 5 5 6 1 9 7 4 2 9 9 0 6 6 1 0 7 1 4 1 7 9 3 9 8 2 0 2 3 0 6 2 2 6 1\n",
      " 3 8 8 0 4 9 3 6 7 9 6 1 5 6 2 5 8]\n",
      "Step: 5251; Loss: 1.04210054874; learn rate: 0.00091762811644\n",
      "Step: 5301; Loss: 0.958337605; learn rate: 0.000916877062991\n",
      "Step: 5351; Loss: 1.03335726261; learn rate: 0.000916126766242\n",
      "saved\n",
      "[8 4 8 6 4 9 4 0 1 4 4 9 0 0 0 7 9 1 8 9 3 9 1 9 5 8 4 9 0 0 8 7 7 6 3 8 3\n",
      " 7 1 0 7 3 7 6 1 8 6 6 0 1 3 1 4 0 7 1 5 9 8 9 3 7 3 8 3 4 1 5 5 9 8 4 6 1\n",
      " 3 3 7 5 8 2 4 9 9 3 8 4 3 3 7 2 6 6 9 5 4 8 8 0 3 9 6 6 2 5 7 1 9 8 0 6 3\n",
      " 7 6 6 8 9 5 2 6 1 2 4 2 6 9 3 4 2]\n",
      "Step: 5401; Loss: 1.03658437729; learn rate: 0.000915376993362\n",
      "Step: 5451; Loss: 1.18035030365; learn rate: 0.000914627918974\n",
      "Step: 5501; Loss: 0.934114277363; learn rate: 0.000913879368454\n",
      "saved\n",
      "[8 6 7 4 5 5 1 8 1 2 4 9 0 9 1 2 7 3 6 4 8 9 9 8 1 6 6 3 1 9 6 8 1 4 3 5 7\n",
      " 6 3 6 3 8 3 0 8 3 2 5 5 9 9 6 6 3 7 4 7 9 4 5 4 5 4 7 9 6 8 4 3 9 0 9 4 4\n",
      " 6 3 0 2 5 0 4 6 2 1 0 6 4 4 0 2 0 0 7 6 3 0 8 8 3 7 1 0 8 2 1 1 3 8 9 7 7\n",
      " 3 1 7 8 4 7 4 3 6 8 0 7 4 6 4 1 9]\n",
      "Step: 5551; Loss: 1.03460431099; learn rate: 0.000913131516427\n",
      "Step: 5601; Loss: 1.14139699936; learn rate: 0.000912384246476\n",
      "Step: 5651; Loss: 0.987480223179; learn rate: 0.000911637616809\n",
      "saved\n",
      "[6 8 5 1 1 8 2 4 6 4 9 8 3 6 1 3 5 0 0 5 1 3 6 6 5 1 0 6 3 2 0 2 0 8 7 3 8\n",
      " 5 2 0 9 4 6 0 4 2 4 3 1 8 9 5 5 9 3 3 3 0 8 0 4 3 1 9 0 3 6 2 8 3 6 2 8 2\n",
      " 8 1 8 7 9 1 7 9 3 3 6 4 5 7 1 0 1 1 2 7 1 2 9 3 1 2 3 8 1 6 3 1 6 9 2 1 8\n",
      " 0 5 0 0 7 9 5 7 6 6 7 7 9 0 1 1 4]\n",
      "Step: 5701; Loss: 1.06728613377; learn rate: 0.00091089156922\n",
      "Step: 5751; Loss: 1.2235172987; learn rate: 0.000910146103706\n",
      "Step: 5801; Loss: 1.17680048943; learn rate: 0.000909401278477\n",
      "saved\n",
      "[0 3 2 1 4 6 9 9 0 3 4 0 4 6 2 7 1 3 6 9 0 3 9 9 5 9 2 8 4 9 6 6 6 6 0 5 6\n",
      " 1 2 8 8 1 1 5 4 5 5 1 2 4 0 6 8 7 0 0 6 7 5 1 9 3 4 6 5 4 5 4 3 5 9 7 4 9\n",
      " 0 1 5 5 5 1 7 8 5 1 8 9 2 5 7 0 9 9 5 7 8 1 6 2 9 6 4 1 6 0 5 2 7 9 9 6 2\n",
      " 5 9 4 5 1 0 9 6 4 3 7 9 8 4 4 4 8]\n",
      "Step: 5851; Loss: 1.04088580608; learn rate: 0.000908657035325\n",
      "Step: 5901; Loss: 1.01143407822; learn rate: 0.000907913490664\n",
      "Step: 5951; Loss: 1.01653945446; learn rate: 0.000907170469873\n",
      "saved\n",
      "[5 3 8 6 2 6 7 3 9 8 4 9 3 6 4 9 1 5 6 6 0 6 5 1 9 1 7 7 6 3 5 9 6 7 0 8 3\n",
      " 4 7 2 2 1 0 0 4 7 2 5 1 0 8 4 4 3 0 3 5 2 5 1 5 5 5 5 3 9 0 7 9 4 3 0 8 8\n",
      " 8 5 5 2 1 1 3 6 1 3 6 7 0 4 5 6 2 5 9 3 4 6 2 7 3 0 8 2 0 7 2 4 5 8 8 6 1\n",
      " 6 0 2 0 2 5 8 3 7 6 4 9 2 2 8 9 0]\n",
      "Step: 6001; Loss: 0.786832213402; learn rate: 0.000906428031158\n",
      "Step: 6051; Loss: 1.05311250687; learn rate: 0.000905686290935\n",
      "Step: 6101; Loss: 0.881869196892; learn rate: 0.000904945132788\n",
      "saved\n",
      "[0 8 4 6 1 4 7 4 1 3 1 1 3 9 0 8 3 0 6 2 1 6 1 8 0 8 0 4 9 2 0 5 0 6 6 9 5\n",
      " 5 3 8 9 0 3 8 0 4 1 1 3 7 5 2 0 3 6 6 5 5 6 9 1 2 4 3 3 5 7 8 2 7 7 2 2 3\n",
      " 7 6 8 1 3 4 5 7 7 4 3 8 7 1 0 7 9 2 7 0 9 5 9 9 9 5 0 1 3 0 6 3 7 9 2 8 4\n",
      " 0 6 6 4 1 6 7 2 6 6 0 1 4 6 7 3 9]\n",
      "Step: 6151; Loss: 0.972795605659; learn rate: 0.000904204556718\n",
      "Step: 6201; Loss: 0.893715858459; learn rate: 0.000903464620933\n",
      "Step: 6251; Loss: 1.05075597763; learn rate: 0.000902725209016\n",
      "saved\n",
      "[2 3 7 6 1 5 1 4 6 1 4 2 9 3 7 9 6 6 1 7 0 8 8 3 3 3 6 1 3 8 6 7 3 1 4 1 2\n",
      " 8 3 4 6 2 2 9 3 9 5 8 7 7 0 2 3 1 2 4 3 5 9 9 4 0 6 3 9 5 9 6 9 6 7 3 2 1\n",
      " 4 0 2 1 2 7 7 2 6 5 7 5 5 5 9 7 0 7 4 4 5 1 6 2 6 8 7 3 4 6 8 9 0 6 0 8 4\n",
      " 5 7 8 8 4 5 5 5 3 8 7 7 8 6 8 1 3]\n",
      "Step: 6301; Loss: 0.986685276031; learn rate: 0.000901986437384\n",
      "Step: 6351; Loss: 1.09397625923; learn rate: 0.000901248306036\n",
      "Step: 6401; Loss: 1.21717357635; learn rate: 0.000900510756765\n",
      "saved\n",
      "[7 0 9 6 1 5 0 0 0 9 3 9 2 6 7 3 6 7 2 6 2 6 0 4 6 7 9 1 4 3 3 1 9 9 8 5 2\n",
      " 0 0 1 1 6 0 6 1 2 6 5 0 9 1 6 8 2 9 3 3 7 6 4 3 5 4 9 1 6 3 3 1 7 6 1 5 8\n",
      " 8 8 9 9 9 9 8 1 3 2 8 6 6 7 2 9 2 5 0 6 7 1 3 1 6 9 9 2 9 4 7 5 8 2 6 2 2\n",
      " 2 7 8 5 2 6 7 5 3 8 3 5 3 8 8 7 7]\n",
      "Step: 6451; Loss: 0.891604542732; learn rate: 0.000899773847777\n",
      "Step: 6501; Loss: 0.954599797726; learn rate: 0.000899037462659\n",
      "Step: 6551; Loss: 1.00471186638; learn rate: 0.000898301776033\n",
      "saved\n",
      "[6 0 9 2 8 8 1 8 8 0 2 6 8 9 5 3 3 6 6 0 7 4 6 6 8 5 6 5 3 6 2 5 8 8 0 8 8\n",
      " 0 2 6 3 6 6 8 4 5 5 4 1 1 1 7 3 9 1 7 0 3 3 9 9 1 1 1 3 2 3 0 0 7 4 7 4 3\n",
      " 4 6 1 5 8 8 9 5 0 0 9 0 9 2 4 0 1 1 4 5 3 0 7 2 5 8 0 6 3 6 5 9 6 4 0 7 2\n",
      " 0 3 7 2 8 2 3 3 7 7 8 9 7 7 2 1 6]\n",
      "Step: 6601; Loss: 0.882383465767; learn rate: 0.000897566613276\n",
      "Step: 6651; Loss: 0.961265563965; learn rate: 0.000896832090802\n",
      "Step: 6701; Loss: 0.956748008728; learn rate: 0.000896098150406\n",
      "saved\n",
      "[3 3 2 7 0 0 8 7 1 5 0 6 5 4 8 4 4 5 0 4 9 8 2 3 4 1 7 1 5 7 3 0 6 1 6 5 2\n",
      " 4 6 9 4 0 3 4 2 3 1 4 2 5 5 8 5 0 8 3 6 6 9 6 9 3 9 2 6 6 0 9 0 3 0 5 2 0\n",
      " 3 0 8 7 3 6 3 2 2 7 6 6 7 8 8 5 7 7 7 7 6 8 7 0 8 9 0 9 1 5 3 6 0 4 8 5 5\n",
      " 9 9 0 5 1 1 7 6 1 4 2 3 0 8 2 6 6]\n",
      "Step: 6751; Loss: 0.922594845295; learn rate: 0.000895364733879\n",
      "Step: 6801; Loss: 0.885755419731; learn rate: 0.000894632015843\n",
      "Step: 6851; Loss: 0.997912168503; learn rate: 0.000893899938092\n",
      "saved\n",
      "[4 0 9 1 2 0 2 3 1 0 0 7 7 9 0 6 1 5 8 1 4 4 0 2 8 9 2 7 7 7 3 7 8 7 6 4 4\n",
      " 9 9 6 0 1 6 2 3 2 6 1 7 4 0 1 1 6 7 9 4 1 1 1 8 3 0 4 7 9 2 8 7 5 0 1 7 8\n",
      " 6 0 2 9 1 2 3 1 1 5 8 0 8 9 3 2 8 5 2 7 3 7 2 1 5 8 3 5 5 8 7 3 5 3 9 4 0\n",
      " 3 0 0 9 5 2 4 8 0 1 1 5 8 3 8 2 1]\n",
      "Step: 6901; Loss: 0.919885993004; learn rate: 0.000893168384209\n",
      "Step: 6951; Loss: 0.941987991333; learn rate: 0.000892437412404\n",
      "Step: 7001; Loss: 0.872159123421; learn rate: 0.000891707080882\n",
      "saved\n",
      "[7 0 7 1 0 9 3 9 3 8 3 8 8 4 5 8 1 2 1 1 5 3 8 8 8 4 7 0 1 6 5 1 2 2 3 7 2\n",
      " 6 1 5 2 7 0 9 6 1 3 9 2 6 6 4 6 5 4 9 1 9 4 0 4 5 7 9 1 1 6 0 4 6 2 5 0 6\n",
      " 5 9 1 9 6 8 6 2 5 0 4 4 5 6 0 8 0 6 3 1 3 8 3 2 9 2 4 0 7 0 5 5 1 3 0 3 2\n",
      " 5 1 7 9 0 1 4 8 6 8 8 4 7 8 2 9 9]\n",
      "Step: 7051; Loss: 0.953955054283; learn rate: 0.000890977389645\n",
      "Step: 7101; Loss: 0.985305905342; learn rate: 0.000890248222277\n",
      "Step: 7151; Loss: 0.78563952446; learn rate: 0.000889519695193\n",
      "saved\n",
      "[3 6 4 7 0 4 0 7 5 9 9 5 5 9 4 8 7 2 8 6 6 9 2 1 0 6 5 3 6 1 9 1 0 3 2 8 5\n",
      " 2 7 7 4 7 7 8 1 9 0 3 6 5 2 1 1 3 7 8 0 0 0 8 4 8 4 0 2 3 5 7 3 3 8 4 9 4\n",
      " 6 4 6 9 3 0 9 2 4 0 8 9 7 0 9 6 6 8 9 2 0 6 2 1 7 4 6 7 0 6 9 9 2 8 4 5 3\n",
      " 1 6 8 9 6 1 9 2 2 3 9 8 7 6 5 2 7]\n",
      "Step: 7201; Loss: 0.855295300484; learn rate: 0.000888791750185\n",
      "Step: 7251; Loss: 0.846601605415; learn rate: 0.000888064445462\n",
      "Step: 7301; Loss: 0.92121887207; learn rate: 0.0008873376064\n",
      "saved\n",
      "[4 6 8 8 3 9 2 6 8 6 9 4 3 3 8 5 5 4 5 1 6 4 4 8 7 4 1 9 2 1 0 8 2 3 4 5 2\n",
      " 5 7 6 9 9 2 3 3 7 0 8 4 7 8 0 3 0 2 5 6 1 5 4 9 9 9 3 8 2 0 0 0 1 0 1 9 4\n",
      " 1 1 4 9 9 6 8 7 0 3 6 0 4 8 0 4 1 2 0 8 7 3 2 8 3 0 2 1 9 3 1 0 1 8 5 7 8\n",
      " 6 4 6 3 5 9 2 6 1 2 5 7 6 3 7 2 1]\n",
      "Step: 7351; Loss: 1.03883898258; learn rate: 0.00088661146583\n",
      "Step: 7401; Loss: 1.00119292736; learn rate: 0.000885885849129\n",
      "Step: 7451; Loss: 0.996404588223; learn rate: 0.00088516093092\n",
      "saved\n",
      "[9 1 3 2 5 1 0 1 4 6 4 8 8 5 8 7 3 7 0 4 6 1 4 0 7 6 9 8 5 1 4 5 9 7 8 1 1\n",
      " 7 1 0 9 0 8 4 6 2 1 1 9 9 1 8 8 1 5 9 2 7 6 8 2 2 9 5 6 4 5 2 7 2 6 8 0 0\n",
      " 4 6 0 6 4 0 5 6 5 9 4 8 8 2 5 5 7 1 7 0 3 1 2 6 6 9 4 0 6 1 9 8 6 4 7 4 1\n",
      " 0 7 3 6 2 7 9 7 7 4 6 4 1 8 6 0 9]\n",
      "Step: 7501; Loss: 1.03953158855; learn rate: 0.00088443653658\n",
      "Step: 7551; Loss: 0.826202869415; learn rate: 0.000883712724317\n",
      "Step: 7601; Loss: 0.987712204456; learn rate: 0.000882989552338\n",
      "saved\n",
      "[9 6 5 7 2 0 7 7 5 0 2 8 6 8 1 9 6 2 2 8 3 0 7 5 6 1 8 3 5 5 4 3 4 7 4 5 0\n",
      " 6 1 7 0 3 6 0 3 2 9 5 9 1 6 9 1 7 4 8 4 8 9 6 5 7 5 7 7 0 0 0 9 2 8 5 7 7\n",
      " 0 4 4 6 6 9 9 2 4 4 9 1 7 3 7 4 4 2 5 0 5 2 1 6 9 4 3 8 5 2 2 8 6 2 9 0 4\n",
      " 9 1 7 0 4 3 7 9 2 9 1 3 1 0 7 6 7]\n",
      "Step: 7651; Loss: 0.99278408289; learn rate: 0.000882266904227\n",
      "Step: 7701; Loss: 1.02103435993; learn rate: 0.000881544954609\n",
      "Step: 7751; Loss: 0.975468873978; learn rate: 0.00088082352886\n",
      "saved\n",
      "[5 5 3 5 1 0 0 3 1 6 2 5 9 1 1 6 4 9 4 4 0 7 2 6 8 2 9 5 7 4 6 2 6 8 2 4 5\n",
      " 1 4 2 3 4 5 4 0 1 2 2 3 5 5 7 2 9 9 2 4 3 7 2 4 7 0 9 0 4 4 9 4 4 8 7 4 0\n",
      " 9 8 6 8 3 6 4 6 8 5 5 7 3 0 1 9 5 8 9 7 1 1 5 1 5 5 3 6 9 3 4 5 3 8 8 7 8\n",
      " 0 5 2 2 7 1 5 4 0 1 9 4 6 3 9 0 4]\n",
      "Step: 7801; Loss: 0.799172759056; learn rate: 0.000880102685187\n",
      "Step: 7851; Loss: 0.928274393082; learn rate: 0.000879382481799\n",
      "Step: 7901; Loss: 0.793403863907; learn rate: 0.000878662802279\n",
      "saved\n",
      "[9 8 8 8 0 8 1 0 8 6 1 1 0 4 2 4 0 5 4 5 3 0 4 0 3 1 0 5 7 5 3 5 7 3 5 1 4\n",
      " 2 9 9 9 9 1 0 8 2 6 4 6 1 2 7 0 1 1 6 8 0 0 9 3 9 9 3 0 5 0 8 5 5 8 5 3 6\n",
      " 2 3 8 6 8 8 9 2 5 8 8 4 9 9 3 3 2 0 9 9 1 6 8 0 5 0 8 3 8 7 1 4 9 5 5 3 6\n",
      " 9 6 9 9 4 6 4 5 5 3 5 0 5 9 2 1 5]\n",
      "Step: 7951; Loss: 0.819007992744; learn rate: 0.000877943763044\n",
      "Step: 8001; Loss: 0.925206184387; learn rate: 0.000877225247677\n",
      "Step: 8051; Loss: 0.977642953396; learn rate: 0.000876507372595\n",
      "saved\n",
      "[6 6 9 9 1 4 4 0 3 5 0 8 4 6 8 8 3 1 4 8 4 9 4 0 3 7 7 4 1 0 3 1 3 5 4 3 5\n",
      " 8 2 7 5 4 7 9 9 1 7 0 4 4 8 3 0 3 9 3 1 0 3 7 5 6 1 5 7 5 1 6 2 7 3 5 6 4\n",
      " 3 1 7 1 1 4 6 2 9 2 4 1 1 7 0 9 9 1 5 6 4 9 3 6 0 2 6 9 9 4 7 8 6 7 0 6 8\n",
      " 7 1 0 7 2 8 2 5 7 3 0 5 1 7 7 4 4]\n",
      "Step: 8101; Loss: 1.16930508614; learn rate: 0.00087579007959\n",
      "Step: 8151; Loss: 1.05959486961; learn rate: 0.000875073368661\n",
      "Step: 8201; Loss: 0.816932439804; learn rate: 0.000874357181601\n",
      "saved\n",
      "[9 6 1 5 5 2 3 5 8 5 2 9 8 3 2 0 6 8 2 8 4 0 8 8 2 4 8 9 2 7 4 2 3 3 3 5 7\n",
      " 8 2 1 7 0 8 3 6 8 8 5 5 3 6 9 4 5 9 0 4 3 6 3 1 7 6 8 4 8 2 4 9 4 7 3 6 5\n",
      " 4 8 7 3 1 1 3 6 1 9 9 9 9 7 6 0 7 2 3 0 4 2 7 4 0 7 5 0 1 4 4 0 5 3 1 3 1\n",
      " 8 6 6 2 9 2 6 7 2 4 3 9 7 9 0 3 3]\n",
      "Step: 8251; Loss: 0.941167116165; learn rate: 0.000873641751241\n",
      "Step: 8301; Loss: 1.00317168236; learn rate: 0.000872926670127\n",
      "Step: 8351; Loss: 0.842572748661; learn rate: 0.000872212345712\n",
      "saved\n",
      "[1 1 8 4 3 5 9 5 8 6 8 1 6 5 8 3 0 2 7 5 1 7 5 8 9 6 5 4 8 2 5 9 0 2 3 7 6\n",
      " 0 1 7 8 3 2 8 1 2 9 1 7 6 2 3 2 9 2 4 0 1 8 6 3 5 8 3 6 2 0 8 9 5 4 9 5 6\n",
      " 5 4 1 6 4 2 8 6 7 9 6 9 1 0 5 0 7 4 5 4 3 0 7 5 3 7 4 8 4 2 2 9 7 7 7 8 9\n",
      " 0 8 3 9 0 3 0 1 0 5 2 3 6 5 8 0 6]\n",
      "Step: 8401; Loss: 0.830851554871; learn rate: 0.000871498545166\n",
      "Step: 8451; Loss: 0.840542793274; learn rate: 0.000870785326697\n",
      "Step: 8501; Loss: 1.15353488922; learn rate: 0.000870072748512\n",
      "saved\n",
      "[1 2 4 3 3 1 7 9 1 7 3 1 2 2 2 6 0 1 6 8 6 4 7 4 5 3 0 1 9 9 1 8 9 2 3 5 3\n",
      " 3 4 3 0 7 0 3 4 9 9 9 5 4 1 7 2 0 9 8 5 8 3 2 8 3 7 8 3 2 5 8 0 4 8 2 7 4\n",
      " 1 5 1 6 8 4 6 9 5 0 9 9 2 2 9 7 3 5 6 5 0 8 5 3 6 2 9 8 0 6 8 3 0 6 1 6 1\n",
      " 0 3 9 1 6 0 1 9 1 3 6 9 1 5 6 9 4]\n",
      "Step: 8551; Loss: 0.8640614748; learn rate: 0.000869360752404\n",
      "Step: 8601; Loss: 0.907905101776; learn rate: 0.000868649221957\n",
      "Step: 8651; Loss: 0.889378488064; learn rate: 0.000867938390002\n",
      "saved\n",
      "[4 3 4 0 6 4 5 7 5 7 1 4 0 3 1 3 3 2 8 5 8 8 6 3 6 7 1 4 8 0 9 1 9 8 9 3 1\n",
      " 3 5 9 4 7 5 9 0 4 0 6 7 0 8 8 3 7 0 7 0 3 0 7 6 7 2 7 4 2 5 4 6 3 7 0 5 3\n",
      " 6 7 4 3 6 0 1 6 9 4 0 0 3 4 2 2 4 2 4 3 3 8 7 9 8 8 3 2 2 4 3 1 4 4 4 9 5\n",
      " 4 5 7 6 1 2 0 7 4 8 7 9 9 5 0 7 0]\n",
      "Step: 8701; Loss: 1.07044196129; learn rate: 0.000867228140123\n",
      "Step: 8751; Loss: 0.826457262039; learn rate: 0.000866518414114\n",
      "Step: 8801; Loss: 0.864850461483; learn rate: 0.000865809328388\n",
      "saved\n",
      "[8 6 3 5 1 5 9 3 9 6 0 4 4 6 6 2 6 3 1 4 9 7 6 5 8 6 7 4 2 9 4 1 3 6 6 9 2\n",
      " 8 6 8 0 0 1 2 2 5 0 4 3 6 9 8 3 5 5 4 4 3 1 5 0 7 5 2 6 4 8 7 2 4 2 6 9 5\n",
      " 4 9 8 4 8 1 0 8 8 7 2 2 7 1 0 2 9 9 3 7 5 0 2 4 0 0 9 1 5 7 4 1 4 7 1 1 2\n",
      " 9 4 0 1 5 7 0 6 5 2 3 7 6 2 5 0 9]\n",
      "Step: 8851; Loss: 1.02638554573; learn rate: 0.000865100708324\n",
      "Step: 8901; Loss: 0.898252725601; learn rate: 0.000864392786752\n",
      "Step: 8951; Loss: 0.932697892189; learn rate: 0.000863685389049\n",
      "saved\n",
      "[2 3 6 7 6 8 7 7 0 2 7 8 2 0 8 2 2 4 1 8 4 9 2 9 2 0 5 1 4 5 8 2 4 3 1 9 5\n",
      " 7 0 8 5 0 3 4 6 5 3 4 0 7 8 9 9 6 2 7 1 9 0 3 2 2 8 6 6 3 1 4 2 5 4 4 3 3\n",
      " 5 1 6 1 8 2 0 1 3 6 7 9 9 2 6 6 9 8 3 0 0 0 1 7 5 3 8 5 4 0 9 9 9 3 4 2 1\n",
      " 5 1 3 9 2 7 5 3 7 1 0 9 3 4 3 7 6]\n",
      "Step: 9001; Loss: 0.947757184505; learn rate: 0.000862978631631\n",
      "Step: 9051; Loss: 1.01437115669; learn rate: 0.000862272339873\n",
      "Step: 9101; Loss: 0.897659420967; learn rate: 0.0008615666884\n",
      "saved\n",
      "[3 4 3 8 3 2 5 4 4 9 9 8 1 1 3 6 1 5 5 8 9 6 2 6 8 6 5 4 9 5 9 5 7 3 3 0 7\n",
      " 3 8 1 7 5 9 4 5 8 8 8 0 4 2 7 2 2 9 0 3 6 8 2 2 0 8 8 8 2 2 8 4 2 1 1 4 0\n",
      " 3 9 2 2 4 6 7 2 4 0 8 8 6 1 3 0 9 7 8 0 3 0 5 7 4 5 0 7 7 1 4 2 4 5 1 1 5\n",
      " 9 4 6 8 0 9 4 5 3 8 2 5 5 9 6 8 9]\n",
      "Step: 9151; Loss: 0.918116271496; learn rate: 0.000860861619003\n",
      "Step: 9201; Loss: 0.997118353844; learn rate: 0.000860157131683\n",
      "Step: 9251; Loss: 0.925711750984; learn rate: 0.00085945322644\n",
      "saved\n",
      "[1 7 0 3 2 2 3 6 5 2 5 5 2 9 5 7 1 2 4 2 3 6 8 3 3 7 7 5 2 5 9 6 3 1 4 4 8\n",
      " 2 3 3 0 5 4 8 7 7 7 2 2 1 4 1 0 5 5 3 8 7 3 8 2 2 9 6 1 6 2 6 9 2 1 8 1 5\n",
      " 1 1 2 0 6 5 1 7 8 4 6 1 0 4 3 8 0 1 1 5 0 2 5 2 9 2 5 4 9 9 9 5 4 1 8 1 0\n",
      " 5 0 6 8 0 2 6 2 5 7 3 3 7 2 8 9 6]\n",
      "Step: 9301; Loss: 0.829904913902; learn rate: 0.000858749903273\n",
      "Step: 9351; Loss: 1.10710191727; learn rate: 0.000858047103975\n",
      "Step: 9401; Loss: 0.848530828953; learn rate: 0.000857344886754\n",
      "saved\n",
      "[6 1 7 5 0 4 7 8 9 3 9 0 5 1 0 2 6 2 0 9 6 2 4 9 4 5 8 2 3 6 0 2 8 5 6 6 0\n",
      " 9 0 1 7 3 0 3 3 4 1 8 9 4 6 0 6 4 2 6 3 8 8 1 0 6 8 7 1 2 8 3 2 4 7 1 2 3\n",
      " 1 0 7 5 1 0 7 9 4 2 6 7 0 7 1 6 5 8 3 3 2 9 0 0 6 1 1 2 3 8 9 3 7 5 8 8 9\n",
      " 1 0 1 3 3 4 4 1 7 1 9 4 5 7 4 8 4]\n",
      "Step: 9451; Loss: 0.768353462219; learn rate: 0.000856643309817\n",
      "Step: 9501; Loss: 0.938357532024; learn rate: 0.000855942314956\n",
      "Step: 9551; Loss: 0.928454756737; learn rate: 0.000855241669342\n",
      "saved\n",
      "[1 2 7 7 8 4 8 2 5 0 5 7 5 3 3 2 6 5 6 6 5 2 2 2 6 3 8 5 7 2 6 8 9 1 6 7 2\n",
      " 2 8 8 6 5 4 0 8 8 0 2 0 0 0 9 8 9 5 9 6 0 7 1 9 5 0 7 0 3 4 2 2 5 1 7 4 2\n",
      " 3 7 8 7 8 0 1 3 5 6 0 9 2 8 3 6 8 7 8 0 6 1 3 0 7 0 5 2 5 9 1 8 7 2 1 4 7\n",
      " 4 3 3 3 9 9 2 0 5 5 0 8 3 2 3 1 4]\n",
      "Step: 9601; Loss: 1.08297038078; learn rate: 0.000854541838635\n",
      "Step: 9651; Loss: 0.965039610863; learn rate: 0.000853842473589\n",
      "Step: 9701; Loss: 0.862644314766; learn rate: 0.000853143748827\n",
      "saved\n",
      "[4 3 8 0 4 8 0 2 4 7 6 1 4 7 2 5 4 4 7 1 7 0 4 9 1 9 0 5 9 6 5 1 4 7 4 3 0\n",
      " 5 3 0 5 8 8 9 7 7 0 3 8 1 1 3 4 8 5 1 9 6 7 7 7 6 8 3 3 7 1 7 6 6 5 2 9 5\n",
      " 3 8 2 7 8 5 9 4 7 2 9 2 5 0 6 4 6 1 2 8 0 1 5 8 5 6 4 3 7 6 0 5 3 7 9 9 1\n",
      " 4 0 7 2 5 4 9 3 6 6 3 3 9 9 8 9 8]\n",
      "Step: 9751; Loss: 0.848403573036; learn rate: 0.00085244566435\n",
      "Step: 9801; Loss: 0.932922720909; learn rate: 0.000851747987326\n",
      "Step: 9851; Loss: 0.898168742657; learn rate: 0.000851050950587\n",
      "saved\n",
      "[4 6 8 0 9 7 8 5 1 2 3 1 6 9 2 1 8 8 5 3 3 5 1 9 9 8 0 6 8 0 5 1 3 4 4 3 4\n",
      " 9 8 2 3 3 9 5 3 9 1 7 3 9 7 5 1 3 1 6 8 8 9 7 0 3 1 7 6 9 8 3 2 4 3 2 6 5\n",
      " 6 3 5 9 6 9 4 6 0 7 5 6 2 3 9 5 4 3 4 4 4 1 6 8 7 9 2 0 0 6 5 4 6 0 7 1 4\n",
      " 5 1 1 3 4 3 3 4 1 1 2 0 3 8 8 8 6]\n",
      "Step: 9901; Loss: 0.910000324249; learn rate: 0.000850354495924\n",
      "Step: 9951; Loss: 0.773965001106; learn rate: 0.00084965856513\n",
      "Step: 10001; Loss: 0.816119194031; learn rate: 0.00084896327462\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1646f66761b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Step: {0}; Loss: {1}; learn rate: {2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.train.start_queue_runners(sess=session)\n",
    "if TRAIN:\n",
    "    while True:\n",
    "        feed_dict = {dropout_keep_prob: 0.7}\n",
    "        _, cur_loss, step, pred_labels, lr = session.run([train_step, loss, global_step, predictions, learn_rate], feed_dict=feed_dict)\n",
    "        if step % 50 == 1:\n",
    "            print \"Step: {0}; Loss: {1}; learn rate: {2}\".format(step, cur_loss, lr)\n",
    "        if step % 150 == 0:\n",
    "            if SAVE:\n",
    "                saver.save(session, save_path + '/model.ckpt', global_step=step)\n",
    "                print 'saved'\n",
    "            print pred_labels\n",
    "elif INFER:\n",
    "    i = 0\n",
    "    session.run(enq)\n",
    "    session.run(close_input_q)\n",
    "    while True:\n",
    "        try:\n",
    "            feed_dict = {dropout_keep_prob: 1}\n",
    "            all_scores, preds = session.run([probs, predictions], feed_dict=feed_dict)\n",
    "            for scores in all_scores:\n",
    "                print \"It's actually a\", INFER_URLS[i]\n",
    "                for cls, score in zip(class_names, scores):\n",
    "                    print '#' * int(1 + 20 * score), cls\n",
    "                i += 1\n",
    "                print '\\n\\n'\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print 'Done!'\n",
    "            break\n",
    "else: # evaluate:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            feed_dict = {dropout_keep_prob: 1}\n",
    "            nc, pred = session.run([n_correct, predictions], feed_dict=feed_dict)\n",
    "            correct += nc\n",
    "            # if i % 10 == 0:\n",
    "            #     print pred\n",
    "            # this assumes all batches are BATCH_SIZE (the last one might be smaller) but that's okay\n",
    "            total += BATCH_SIZE\n",
    "            i += 1\n",
    "            if i % 10 == 0:\n",
    "                print \"Accuracy so far: {0}\".format(correct * 1.0 / total)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print 'Done!'\n",
    "            print \"Final accuracy: {0}\".format(correct * 1.0 / total)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
