{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "global_step = tf.contrib.framework.get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "BATCH_SIZE = 32\n",
    "REVIEW_LEN = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-4, started daemon 123145559244800)>,\n",
       " <Thread(Thread-5, started daemon 123145563451392)>,\n",
       " <Thread(Thread-6, started daemon 123145567657984)>,\n",
       " <Thread(Thread-7, started daemon 123145571864576)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_and_decode_single_example(filename, epochs=None):\n",
    "    # first construct a queue containing a list of filenames.\n",
    "    # this lets a user split up there dataset in multiple files to keep\n",
    "    # size down\n",
    "    filename_queue = tf.train.string_input_producer([filename], num_epochs=epochs)\n",
    "    # Unlike the TFRecordWriter, the TFRecordReader is symbolic\n",
    "    reader = tf.TFRecordReader()\n",
    "    # One can read a single serialized example from a filename\n",
    "    # serialized_example is a Tensor of type string.\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    # The serialized example is converted back to actual values.\n",
    "    # One needs to describe the format of the objects to be returned\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            # We know the length of both fields. If not the\n",
    "            # tf.VarLenFeature could be used\n",
    "            'stars': tf.FixedLenFeature([], tf.int64),\n",
    "            'text': tf.FixedLenFeature([], 'string')\n",
    "        })\n",
    "    # now return the converted data\n",
    "    stars = features['stars']\n",
    "    chars = tf.decode_raw(features['text'], tf.uint8)\n",
    "    chars = tf.reshape(chars, [REVIEW_LEN])\n",
    "    return stars, chars\n",
    "\n",
    "# load training batcher:\n",
    "filename = 'yelp_train.tfrecords'\n",
    "stars, chars = read_and_decode_single_example(filename, epochs=(None if TRAIN else 1))\n",
    "stars_batch, chars_batch = tf.train.shuffle_batch([stars, chars], batch_size=BATCH_SIZE, capacity=1000, min_after_dequeue=500)\n",
    "\n",
    "# load testing batcher:\n",
    "test_stars, test_chars = read_and_decode_single_example('yelp_test.tfrecords', epochs=None)\n",
    "TEST_BATCHES = 10 * 1000 / BATCH_SIZE\n",
    "stars_batch_test, chars_batch_test = tf.train.batch([test_stars, test_chars], batch_size=BATCH_SIZE, allow_smaller_final_batch=True)\n",
    "\n",
    "tf.train.start_queue_runners(sess=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print session.run(stars_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_keep_prob = tf.placeholder_with_default(tf.constant(1.0), [], name='dropout_keep_prob')        \n",
    "\n",
    "def weight_var(shape, stddev=0.1, weight_decay=0, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    v = tf.Variable(initial, name=name)\n",
    "    if weight_decay > 0:\n",
    "        l2 = tf.nn.l2_loss(v) * weight_decay\n",
    "        tf.add_to_collection('losses', l2)\n",
    "    return v\n",
    "\n",
    "def create_fc(input, out_size, relu=True):\n",
    "    # input_dropped = tf.nn.dropout(input, dropout_keep_prob)\n",
    "    in_size = input.get_shape()[-1].value\n",
    "    w = weight_var([in_size, out_size], weight_decay=0.004, name='w')\n",
    "    b = weight_var([out_size], weight_decay=0.004, name='b')\n",
    "    x = tf.matmul(input, w)\n",
    "    return tf.nn.relu(x + b) if relu else x + b\n",
    "\n",
    "def create_conv(input, out_channels, patch_size=5, stride=1, batch_norm=False, dropout=False):\n",
    "    in_channels = input.get_shape()[-1].value\n",
    "    w = weight_var([patch_size, patch_size, in_channels, out_channels], name='w')\n",
    "    b = weight_var([out_channels], stddev=0, name='b')\n",
    "    conv = tf.nn.conv2d(input, w, strides=[1,stride,stride,1], padding='SAME')\n",
    "    if batch_norm: conv = create_batch_norm(conv)\n",
    "    activation = tf.nn.relu(conv + b)\n",
    "    if dropout: activation = create_dropout(activation)\n",
    "    return activation\n",
    "\n",
    "def create_max_pool(inputs, ksize=2, stride=2):\n",
    "    return tf.nn.max_pool(inputs, ksize=[1, ksize, ksize, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def create_batch_norm(inputs):\n",
    "    return batch_norm(inputs, is_training=TRAIN)\n",
    "\n",
    "def create_dropout(inputs):\n",
    "    return tf.nn.dropout(inputs, dropout_keep_prob)\n",
    "\n",
    "def text_conv(input, out_channels, patch_size=5, stride=1, pool_size=1):\n",
    "    in_channels = input.get_shape()[-1].value\n",
    "    w = weight_var([patch_size, in_channels, out_channels])\n",
    "    b = weight_var([out_channels], stddev=0)\n",
    "    conv = tf.nn.conv1d(input, w, stride=stride, padding='SAME')\n",
    "    activation = tf.nn.relu(conv + b)\n",
    "    return activation\n",
    "\n",
    "def text_pool(inputs, ksize=2, stride=2, type='avg'):\n",
    "    channels = inputs.get_shape()[-1].value\n",
    "    length = inputs.get_shape()[-2].value\n",
    "    inputs = tf.reshape(inputs, [-1, 1, length, channels])\n",
    "    fn = {'avg': tf.nn.avg_pool, 'max': tf.nn.max_pool}[type]\n",
    "    pooled =  fn(inputs, ksize=[1, 1, ksize, 1], strides=[1, 1, stride, 1], padding='SAME')\n",
    "    return tf.reshape(pooled, [-1, length / stride, channels])\n",
    "\n",
    "def flatten_tensor(t):\n",
    "    shape = [s.value for s in t.get_shape()]\n",
    "    flat_size = 1\n",
    "    for x in shape[1:]:\n",
    "        flat_size *= x\n",
    "    return tf.reshape(t, [-1, flat_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review = tf.placeholder_with_default(chars_batch, [None, REVIEW_LEN], name='review')\n",
    "\n",
    "def conv_model(review):\n",
    "    one_hot = tf.one_hot(review, 255, dtype=tf.float32)\n",
    "    conv0 = text_conv(one_hot, 84, patch_size=1)\n",
    "    conv1 = text_conv(conv0, 128, patch_size=3)\n",
    "    conv2 = text_conv(conv1, 64, patch_size=5)\n",
    "    conv2_dropped = create_dropout(conv2)\n",
    "    pool3 = text_pool(conv2_dropped, ksize=4, stride=4) # [?, 250, 64]\n",
    "    conv4 = text_conv(pool3, 32, patch_size=5)\n",
    "    pool5 = text_pool(conv4, ksize=10, stride=10) # [?, 25, 32]\n",
    "    pool5_flat = tf.reshape(pool5, [-1, pool5.get_shape()[-2].value * pool5.get_shape()[-1].value])\n",
    "    fc6 = create_fc(pool5_flat, 64)\n",
    "    fc7 = create_fc(fc6, 1, relu=False)\n",
    "    return fc7\n",
    "\n",
    "def fc_model(review):\n",
    "    one_hot = tf.one_hot(review, 255, dtype=tf.float32)\n",
    "    conv1 = text_conv(one_hot, 4)\n",
    "    conv1_flat = tf.reshape(conv1, [-1, REVIEW_LEN * 4])\n",
    "    fc2 = create_fc(conv1_flat, 32)\n",
    "    return create_fc(fc2, 1, relu=False)\n",
    "\n",
    "def small_conv_1(review):\n",
    "    one_hot = tf.one_hot(review, 255, dtype=tf.float32)\n",
    "    conv0 = text_conv(one_hot, 64, patch_size=1)\n",
    "    conv1 = text_conv(conv0, 16, patch_size=9)\n",
    "    conv1 = create_dropout(conv1)\n",
    "    conv2 = text_conv(conv1, 4, patch_size=9)\n",
    "    pool3 = text_pool(conv2, ksize=100, stride=100) # 10x4\n",
    "    fc3 = create_fc(flatten_tensor(conv2), 8)\n",
    "    return create_fc(flatten_tensor(fc3), 1, relu=False)\n",
    "\n",
    "def conv_model_2(review):\n",
    "    one_hot = tf.one_hot(review, 255, dtype=tf.float32)\n",
    "    conv0 = text_conv(one_hot, 64, patch_size=1) # 1000x64 -- embed letters into 64d\n",
    "    conv1 = text_conv(conv0, 256, patch_size=7) # 1000x256 -- identify 256 words ~7 chars long\n",
    "    conv1 = create_dropout(conv1)\n",
    "    pool2 = text_pool(conv1, ksize=5, stride=5, type='max') # 200x256 -- reduce\n",
    "    conv3 = text_conv(pool2, 32, patch_size=5) # 200x128 -- identify positive and negative word sequences\n",
    "    pool4 = text_pool(conv3, ksize=50, stride=50, type='max') # 4x32 -- reduce\n",
    "    flat5 = flatten_tensor(pool4)\n",
    "    fc6 = create_fc(flat5, 16)\n",
    "    return create_fc(fc6, 1, relu=False)\n",
    "\n",
    "predicted_stars = conv_model_2(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "star_rating = tf.placeholder_with_default(stars_batch, [None], name='stars')\n",
    "loss = tf.reduce_sum(tf.nn.l2_loss(tf.cast(star_rating, tf.float32) - predicted_stars))\n",
    "\n",
    "# compute accuracy:\n",
    "which_correct = tf.equal(tf.cast(star_rating, tf.float32), tf.round(predicted_stars))\n",
    "accuracy = tf.reduce_mean(tf.cast(which_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learn_rate = tf.placeholder_with_default(tf.constant(0.01), [], name='learn_rate')\n",
    "opt = tf.train.AdamOptimizer(learn_rate)\n",
    "learn_step = opt.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if hasattr(tf, 'global_variables_initializer'):\n",
    "    session.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "else:\n",
    "    session.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss: 5924.87792969, accuracy: 0.0\n",
      "step: 6, loss: 1807.52001953, accuracy: 0.0625\n",
      "step: 11, loss: 890.352600098, accuracy: 0.375\n",
      "step: 16, loss: 682.052062988, accuracy: 0.375\n",
      "step: 21, loss: 1046.1640625, accuracy: 0.09375\n",
      "step: 26, loss: 1295.3092041, accuracy: 0.25\n",
      "step: 31, loss: 962.23828125, accuracy: 0.3125\n",
      "step: 36, loss: 998.872558594, accuracy: 0.15625\n",
      "step: 41, loss: 648.694702148, accuracy: 0.24609375\n",
      "step: 46, loss: 833.254882812, accuracy: 0.1875\n",
      "step: 51, loss: 1847.66589355, accuracy: 0.283203125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e1488882abdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlearn_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     }\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstars_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlearn_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_stars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep_\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"step: {}, loss: {}, accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nateparrott/Documents/School/deep/hw/venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nateparrott/Documents/School/deep/hw/venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nateparrott/Documents/School/deep/hw/venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/nateparrott/Documents/School/deep/hw/venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nateparrott/Documents/School/deep/hw/venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    feed_dict = {\n",
    "        dropout_keep_prob: 0.66,\n",
    "        learn_rate: 0.01\n",
    "    }\n",
    "    _, step_, loss_, acc_, stars_ = session.run([learn_step, global_step, loss, accuracy, predicted_stars], feed_dict=feed_dict)\n",
    "    if step_ % 5 == 1:\n",
    "        print \"step: {}, loss: {}, accuracy: {}\".format(step_, loss_, acc_)\n",
    "        # print stars_\n",
    "    if step_ % 5 == 1 and False:\n",
    "        # evaluate:\n",
    "        accuracies = []\n",
    "        for i in xrange(TEST_BATCHES):\n",
    "            chars_in, stars_in = session.run([chars_batch_test, stars_batch_test])\n",
    "            feed = {\n",
    "                review: chars_in,\n",
    "                star_rating: stars_in\n",
    "            }\n",
    "            acc_ = session.run([accuracy], feed_dict=feed)\n",
    "            accuracies.append(acc_)\n",
    "            print acc_, i, TEST_BATCHES\n",
    "        print 'Test accuracy:', sum(accuracies) * 1.0 / len(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
