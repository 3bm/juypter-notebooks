{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = set()\n",
    "for filename in ['male.txt', 'female.txt']:\n",
    "    for line in open(os.path.join('../data/names', filename)):\n",
    "        if len(line.strip()):\n",
    "            names.add(line.strip().lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7583 names\n",
      "longest: 54\n",
      "99th percentile longest: 9\n"
     ]
    }
   ],
   "source": [
    "print len(names), 'names'\n",
    "print 'longest:', max(map(len, names))\n",
    "by_len = sorted(names, key=len)\n",
    "print '99th percentile longest:', len(by_len[int(len(names) * 0.95)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13  0 19  4 26 27 27 27 27 27]\n"
     ]
    }
   ],
   "source": [
    "chars = list('abcdefghijklmnopqrstuvwxyz') + ['<END>', '<NULL>']\n",
    "indices_for_chars = {c: i for i, c in enumerate(chars)}\n",
    "\n",
    "NAME_MAX_LEN = 10 # include the <END> char\n",
    "\n",
    "def name_to_vec(name, maxlen=NAME_MAX_LEN):\n",
    "    v = np.zeros(maxlen, dtype=int)\n",
    "    v.fill(indices_for_chars.get('<NULL>'))\n",
    "    for i, c in enumerate(name):\n",
    "        if i >= maxlen: break\n",
    "        n = indices_for_chars.get(c, '<NULL>')\n",
    "        v[i] = n\n",
    "    v[min(len(name), maxlen-1)] = indices_for_chars['<END>']\n",
    "    return v\n",
    "\n",
    "def vec_to_name(vec):\n",
    "    name = ''\n",
    "    for x in vec:\n",
    "        char = chars[x]\n",
    "        if len(char) == 1:\n",
    "            name += char\n",
    "        elif char == '<END>':\n",
    "            return name\n",
    "    return name\n",
    "\n",
    "print name_to_vec('nate')\n",
    "assert vec_to_name(name_to_vec('nate')) == 'nate'\n",
    "assert vec_to_name(name_to_vec('aaaaaaaaaaaa')) == 'aaaaaaaaa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NOISE_SIZE = 32\n",
    "noise = tf.placeholder(shape=[None, NOISE_SIZE], dtype=tf.float32, name='noise')\n",
    "real_names = tf.placeholder(shape=[None, NAME_MAX_LEN], dtype=tf.int32, name='names')\n",
    "\n",
    "batch_shape = tf.reshape(tf.shape(real_names)[0], [-1])\n",
    "use_real_name = tf.random_uniform(shape=batch_shape, minval=0, maxval=1, dtype=tf.int32, seed=None, name='use_real_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_var(shape, stddev=0.1, weight_decay=0, name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    v = tf.Variable(initial, name=name)\n",
    "    if weight_decay > 0:\n",
    "        l2 = tf.nn.l2_loss(v) * weight_decay\n",
    "        tf.add_to_collection('losses', l2)\n",
    "    return v\n",
    "\n",
    "def leaky_relu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "def relu(x):\n",
    "    # return tf.nn.relu(x)\n",
    "    return leaky_relu(x)\n",
    "\n",
    "def create_conv(input, out_channels, patch_size=5, stride=1, batch_norm=False, dropout=False):\n",
    "    in_channels = input.get_shape()[-1].value\n",
    "    w = weight_var([patch_size, patch_size, in_channels, out_channels])\n",
    "    b = weight_var([out_channels], stddev=0)\n",
    "    conv = tf.nn.conv2d(input, w, strides=[1,stride,stride,1], padding='SAME')\n",
    "    if batch_norm: conv = create_batch_norm(conv)\n",
    "    activation = relu(conv + b)\n",
    "    if dropout: activation = create_dropout(activation)\n",
    "    return activation\n",
    "    \n",
    "def text_conv(input, out_channels, patch_size=5, stride=1, dropout=False, pool_size=1):\n",
    "    in_channels = input.get_shape()[-1].value\n",
    "    w = weight_var([patch_size, in_channels, out_channels])\n",
    "    b = weight_var([out_channels], stddev=0)\n",
    "    conv = tf.nn.conv1d(input, w, stride=stride, padding='SAME')\n",
    "    activation = relu(conv + b)\n",
    "    # TODO: max_pooling\n",
    "    if dropout: activation = create_dropout(activation)\n",
    "    return activation\n",
    "\n",
    "def create_dropout(units):\n",
    "    return tf.nn.dropout(units, dropout)\n",
    "\n",
    "def create_fc(input, out_size):\n",
    "    # input_dropped = tf.nn.dropout(input, dropout_keep_prob)\n",
    "    in_size = input.get_shape()[-1].value\n",
    "    w = weight_var([in_size, out_size], weight_decay=0.004)\n",
    "    b = weight_var([out_size], weight_decay=0.004)\n",
    "    x = tf.matmul(input, w)\n",
    "    return relu(x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generator(noise, name='generator'):\n",
    "    with tf.variable_scope(name, reuse=None):\n",
    "        cells = [tf.nn.rnn_cell.LSTMCell(size, state_is_tuple=True) for size in [NOISE_SIZE, 256, len(chars)]]\n",
    "        lstm = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
    "        noise_repeated_over_time = tf.tile(tf.reshape(noise, [-1, 1, NOISE_SIZE]), [1, NAME_MAX_LEN, 1])\n",
    "        outputs, state = tf.nn.dynamic_rnn(lstm, noise_repeated_over_time, dtype=tf.float32)\n",
    "        output_chars = tf.reshape(tf.argmax(tf.nn.softmax(outputs), axis=2), [-1, NAME_MAX_LEN])\n",
    "        output_chars = tf.cast(output_chars, tf.int32)\n",
    "    return output_chars\n",
    "\n",
    "generated_names = generator(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yyyyyyykkk\n",
      "qqwwwnnnnn\n",
      "tttttttttt\n",
      "zzzzzzzzzz\n",
      "ddddddvvvv\n",
      "ffffuuuuuu\n",
      "iiiiiiiiii\n",
      "hhhh\n",
      "ggtttttttt\n",
      "gggggajjjj\n"
     ]
    }
   ],
   "source": [
    "def generate_noise(n=1):\n",
    "    return np.random.normal(size=(n, NOISE_SIZE))\n",
    "\n",
    "generated = session.run(generated_names, feed_dict={noise: generate_noise(n=10)})\n",
    "for v in generated:\n",
    "    print vec_to_name(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def discriminator(names, name='discriminator'):\n",
    "     with tf.variable_scope(name, reuse=None):\n",
    "            one_hot = tf.one_hot(names, len(chars), dtype=tf.float32)\n",
    "            conv1 = text_conv(one_hot, 64)\n",
    "            conv2 = text_conv(one_hot, 32)\n",
    "            fc1 = create_fc(tf.reshape(conv2, [-1, NAME_MAX_LEN * 32]), 32)\n",
    "            fc2 = create_fc(fc1, 2)\n",
    "            is_real = tf.unpack(tf.nn.softmax(fc2), axis=1)[0]\n",
    "            return is_real\n",
    "#             cells = [tf.nn.rnn_cell.LSTMCell(size, state_is_tuple=True) for size in [len(chars), 32]]\n",
    "#             lstm = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
    "#             outputs, state = tf.nn.dynamic_rnn(lstm, one_hot, dtype=tf.float32)\n",
    "#             outputs_flat = tf.reshape(outputs, [-1, 32 * NAME_MAX_LEN])\n",
    "disc_input = real_names * use_real_name + generated_names * (tf.cast(1, tf.int32) - use_real_name)\n",
    "guessed_real = discriminator(disc_input)\n",
    "disc_loss = tf.reduce_sum(tf.cast(use_real_name, tf.float32) * -tf.log(guessed_real))\n",
    "disc_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(guessed_real), tf.cast(use_real_name, tf.float32)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(\"generator/RNN/MultiRNNCell/Cell0/LSTMCell/W_0/read:0\", shape=(64, 128), dtype=float32)', 'Tensor(\"generator/RNN/MultiRNNCell/Cell0/LSTMCell/B/read:0\", shape=(128,), dtype=float32)', 'Tensor(\"generator/RNN/MultiRNNCell/Cell1/LSTMCell/W_0/read:0\", shape=(288, 1024), dtype=float32)', 'Tensor(\"generator/RNN/MultiRNNCell/Cell1/LSTMCell/B/read:0\", shape=(1024,), dtype=float32)', 'Tensor(\"generator/RNN/MultiRNNCell/Cell2/LSTMCell/W_0/read:0\", shape=(284, 112), dtype=float32)', 'Tensor(\"generator/RNN/MultiRNNCell/Cell2/LSTMCell/B/read:0\", shape=(112,), dtype=float32)'] and loss Tensor(\"Neg_3:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-72d5a3c59d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisc_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    274\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables ['Tensor(\"generator/RNN/MultiRNNCell/Cell0/LSTMCell/W_0/read:0\", shape=(64, 128), dtype=float32)', 'Tensor(\"generator/RNN/MultiRNNCell/Cell0/LSTMCell/B/read:0\", shape=(128,), dtype=float32)', 'Tensor(\"generator/RNN/MultiRNNCell/Cell1/LSTMCell/W_0/read:0\", shape=(288, 1024), dtype=float32)', 'Tensor(\"generator/RNN/MultiRNNCell/Cell1/LSTMCell/B/read:0\", shape=(1024,), dtype=float32)', 'Tensor(\"generator/RNN/MultiRNNCell/Cell2/LSTMCell/W_0/read:0\", shape=(284, 112), dtype=float32)', 'Tensor(\"generator/RNN/MultiRNNCell/Cell2/LSTMCell/B/read:0\", shape=(112,), dtype=float32)'] and loss Tensor(\"Neg_3:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "\n",
    "learn_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "optimizer = tf.train.AdamOptimizer(learn_rate)\n",
    "global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "train_disc = optimizer.minimize(disc_loss, global_step=global_step, var_list=disc_vars)\n",
    "train_gen = optimizer.minimize(-disc_loss, global_step=global_step, var_list=gen_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
